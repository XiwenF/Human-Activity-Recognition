{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzFjeCdP/XAJJNfIar9WjZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import** **Module** **and data**"
      ],
      "metadata": {
        "id": "Y-wePfkc-bwY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T2oRaTPnim8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from numpy import savetxt\n",
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Support Vector Machines\n",
        "from sklearn.svm import LinearSVC\n",
        "# Decision Trees\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "# K-Nearest Neighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from matplotlib import pyplot\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import xgboost as xgb\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from   sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import  KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "dataset = pd.read_csv('training.csv',engine='python' )\n",
        "dataset_test = pd.read_csv('test.csv',engine='python' )\n",
        "dataset.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary Classification** \n",
        "\n",
        "Classify the activity into static(0) and dynamic(1)\n",
        "\n",
        "The following models are included:  \n",
        "\n",
        "(1) Support Vector Machine with kenel: linear, poly, rbf, sigmoid. \n",
        "\n",
        "(2) Decision Tree \n",
        "\n",
        "(3) Guassian Naive Bayes \n",
        "\n",
        "(4) K-Nearest Neighbors \n",
        "\n",
        "(5) Logistic Regression \n",
        "\n",
        "(6) Linear Discriminant Analysis \n",
        "\n",
        "(7) Quadratic Discriminant Analysis  \n",
        "\n",
        "(8) Random Forest  \n",
        "\n",
        "(9) Voting Classifier \n",
        "\n",
        "(10) Neutral Network \n",
        "\n",
        "(11) Bagging Classifier  \n",
        "\n",
        "(12) AdaBoost Classifier \n",
        "\n",
        "(13) Gaussian Process Classifier with RBF kernel  \n",
        "\n",
        "(14) SGD Classifier \n",
        "\n",
        "(15) Multinomial Logistic Regression"
      ],
      "metadata": {
        "id": "1Z3_TlngGzQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Enable Data**"
      ],
      "metadata": {
        "id": "OYqvbr9XHTlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "List0 = ['4','5','6']\n",
        "List1 = ['1','2','3']\n",
        "List2 = ['7','8','9','10','11','12']\n",
        "dataset[\"V0\"] = np.where(dataset['V2'].isin(List1),'1','0')\n",
        "ytrain = dataset[\"V0\"]\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "m00jyX8GnlFS",
        "outputId": "ef88aa8b-877b-4762-8780-0666f3044d1e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e9bdf0ab31cc>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mList1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mList2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'7'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'9'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'10'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'11'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'12'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"V0\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'V2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mytrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"V0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain=dataset.iloc[1:,3:564]\n",
        "xtrain\n",
        "ytrain=ytrain.iloc[1:]\n",
        "dataset_test['V2']=0\n",
        "dataset_test_x = pd.read_csv('xtest.csv',engine='python' )\n",
        "dataset_test_x\n",
        "xtest2=dataset_test_x.iloc[0:,1:]\n",
        "xtest2\n",
        "dataset_test.head()\n",
        "models = {}\n",
        "xtrain=dataset.iloc[1:,3:564]\n",
        "\n",
        "dataset_test_x = pd.read_csv('xtest.csv',engine='python' )"
      ],
      "metadata": {
        "id": "TXlFcIqenpAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Split Data**"
      ],
      "metadata": {
        "id": "yAp2dDzhIZZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I tried several ways to split the data, including 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4. \n",
        "#Considering the limited space here, this version of code only including the 0.4, \n",
        "#which meeans divide the data into train(60%) and test(40%) data. \n",
        "#You can easily change the fraction as you want.\n",
        "X_train31, X_test31, y_train31, y_test31 = train_test_split(xtrain, ytrain , test_size=0.4, random_state=0)"
      ],
      "metadata": {
        "id": "bPl5Vg9vozq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Build base model**"
      ],
      "metadata": {
        "id": "ryN3qxfaIAJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Estimator\n",
        "dummy = DummyClassifier(strategy='stratified')\n",
        "dummy.fit(xtrain,ytrain)\n",
        "\n",
        "# Check for Model Accuracy\n",
        "dummy.score(xtrain,ytrain)"
      ],
      "metadata": {
        "id": "3Yi4Kcp0oRjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Find the opimal parameters**"
      ],
      "metadata": {
        "id": "avYVaLYDM-Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for random_state\n",
        "param_grid = {'random_state': range(0, 101)}\n",
        "\n",
        "# Define the SVM classifier\n",
        "svc = SVC()\n",
        "\n",
        "# Define the grid search object\n",
        "grid_search = GridSearchCV(svc, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search object to the data\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# Print the best random_state parameter value and the corresponding mean test score\n",
        "print('Best random_state:', grid_search.best_params_['random_state'])\n",
        "print('Mean test score:', grid_search.best_score_)"
      ],
      "metadata": {
        "id": "svp2oaO6o2XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create decision tree classifier\n",
        "dtc = DecisionTreeClassifier()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 4, 6, 8, 10],\n",
        "    'min_samples_split': [2, 4, 6, 8],\n",
        "    'min_samples_leaf': [1, 2, 3, 4]\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(dtc, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "Ayc5CilGpRsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(lr, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "kAifWooYpd6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create LDA classifier\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'solver': ['svd', 'lsqr'],\n",
        "    'shrinkage': [None, 'auto', 0.1, 0.5, 0.9],\n",
        "    'n_components': [1, 2, 3]\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(lda, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "z91-ZqtUpj5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "MELY4Lbcp-FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create QDA classifier\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'reg_param': [0.0, 0.1, 0.5, 0.9]\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(qda, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "R4W4sX1Fp5Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 20],\n",
        "    'class_weight': [None, 'balanced'],\n",
        "    'n_jobs': [-1, 1, 2],\n",
        "    'random_state': [0,42, 123, 456]\n",
        "}\n",
        "\n",
        "# Create the random forest classifier\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Perform the grid search using cross-validation\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# Print the best hyperparameters and the corresponding validation score\n",
        "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Validation score: {grid_search.best_score_:.2f}\")"
      ],
      "metadata": {
        "id": "U12fhU7iquQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameter grid\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100], 'kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
        "\n",
        "# create SVM classifier\n",
        "svc = SVC()\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "jxJrGr1uq6dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Training data evaluation**"
      ],
      "metadata": {
        "id": "WQ1eFk6uOkuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build multiple classifiers\n",
        "svm0 = LinearSVC()\n",
        "dt = DecisionTreeClassifier( max_depth=4, min_samples_leaf= 2,min_samples_split= 4)\n",
        "nb = GaussianNB()\n",
        "knn = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\n",
        "lr = LogisticRegression(C= 1, penalty= 'l2', solver='liblinear')\n",
        "# adab = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100)\n",
        "lda = LinearDiscriminantAnalysis(n_components=1, shrinkage=None, solver='svd')\n",
        "qda = QuadraticDiscriminantAnalysis(reg_param= 0.1)\n",
        "#xgb = xgb.XGBClassifier()\n",
        "svm2 = SVC(random_state=0)\n",
        "clf = RandomForestClassifier(max_depth=10, n_jobs=-1, random_state=0)\n",
        "svm3 = SVC(C=0.1,gamma=0.1,kernel='linear')\n",
        "svm4 = SVC(kernel='poly')\n",
        "svm5 = SVC(kernel='rbf')\n",
        "svm6 = SVC(kernel='sigmoid')\n",
        "# Define the ensemble classifier\n",
        "ensemble = VotingClassifier(estimators=[('dt', svm0), ('lr', svm3), ('svm', svm4)], voting='hard')\n",
        "# Define the ensemble classifier\n",
        "ensemble2 = VotingClassifier(estimators=[('dt', dt ), ('lr', lr), ('knn', knn ), ('lda', lda), ('qda', qda),('svm6', svm6), ('nb', nb),('clf', clf),('svm0', svm0) ,('svm2', svm2),('svm3',svm3),('svm4', svm4 )], voting='hard')\n",
        "ensemble3 = VotingClassifier(estimators=[('dt', dt ), ('lr', lr), ('svm', svm0),('nb',nb),('knn', knn ), ('lda', lda), ('qda', qda), ('svm2', svm2),('svm3',svm3),('svm4', svm4 ), ('svm5', svm5), ('svm6', svm6)], voting='hard')\n",
        "\n",
        "# Train the classifiers on the training set\n",
        "svm0.fit(X_train31, y_train31)\n",
        "svm3.fit(X_train31, y_train31)\n",
        "svm4.fit(X_train31, y_train31)\n",
        "svm5.fit(X_train31, y_train31)\n",
        "svm6.fit(X_train31, y_train31)\n",
        "\n",
        "dt.fit(X_train31, y_train31)\n",
        "nb.fit(X_train31, y_train31)\n",
        "knn.fit(X_train31, y_train31)\n",
        "lr.fit(X_train31, y_train31)\n",
        "#adab.fit(X_train32, y_train32)\n",
        "lda.fit(X_train31, y_train31)\n",
        "qda.fit(X_train31, y_train31)\n",
        "#xgb.fit(X_train31, y_train31)\n",
        "clf.fit(X_train31, y_train31)\n",
        "ensemble.fit(X_train31, y_train31)\n",
        "ensemble2.fit(X_train31, y_train31)\n",
        "ensemble3.fit(X_train31, y_train31)\n",
        "# Predict the test set using the trained classifiers\n",
        "svm0_pred = svm0.predict(X_test31)\n",
        "svm3_pred = svm3.predict(X_test31)\n",
        "svm4_pred = svm4.predict(X_test31)\n",
        "svm5_pred = svm5.predict(X_test31)\n",
        "svm6_pred = svm6.predict(X_test31)\n",
        "\n",
        "dt_pred = dt.predict(X_test31)\n",
        "nb_pred = nb.predict(X_test31)\n",
        "knn_pred = knn.predict(X_test31)\n",
        "lr_pred = lr.predict(X_test31)\n",
        "#adab_pred = adab.predict(X_test32)\n",
        "lda_pred = lda.predict(X_test31)\n",
        "qda_pred = qda.predict(X_test31)\n",
        "#xgb_pred = xgb.predict(X_test31)\n",
        "clf_pred = clf.predict(X_test31)\n",
        "ensemble_pred = ensemble.predict(X_test31)\n",
        "ensemble_pred2 = ensemble2.predict(X_test31)\n",
        "ensemble_pred3 = ensemble3.predict(X_test31)\n",
        "# Evaluate the accuracy of each classifier on the test set\n",
        "svm0_acc = accuracy_score(y_test31, svm0_pred)\n",
        "svm3_acc = accuracy_score(y_test31, svm3_pred)\n",
        "svm4_acc = accuracy_score(y_test31, svm4_pred)\n",
        "svm5_acc = accuracy_score(y_test31, svm5_pred)\n",
        "svm6_acc = accuracy_score(y_test31, svm6_pred)\n",
        "\n",
        "dt_acc = accuracy_score(y_test31, dt_pred)\n",
        "nb_acc = accuracy_score(y_test31, nb_pred)\n",
        "knn_acc = accuracy_score(y_test31, knn_pred)\n",
        "lr_acc = accuracy_score(y_test31, lr_pred)\n",
        "#adab_acc = accuracy_score(y_test32, adab_pred)\n",
        "lda_acc = accuracy_score(y_test31, lda_pred)\n",
        "qda_acc = accuracy_score(y_test31, qda_pred)\n",
        "#xgb_acc = accuracy_score(y_test31, xgb_pred)\n",
        "clf_acc = accuracy_score(y_test31, clf_pred)\n",
        "ensemble_acc = accuracy_score(y_test31, ensemble_pred)\n",
        "ensemble_acc2 = accuracy_score(y_test31, ensemble_pred2)\n",
        "ensemble_acc3 = accuracy_score(y_test31, ensemble_pred3)\n",
        "\n",
        "print('svm0 Classifier accuracy:', svm0_acc)\n",
        "print('svm3 Classifier accuracy:', svm3_acc)\n",
        "print('svm4 Classifier accuracy:', svm4_acc)\n",
        "print('svm5 Classifier accuracy:', svm5_acc)\n",
        "print('svm6 Classifier accuracy:', svm6_acc)\n",
        "\n",
        "print('dt Classifier accuracy:', dt_acc)\n",
        "print('nb Classifier accuracy:', nb_acc)\n",
        "print('K-Nearest Neighbors Classifier accuracy:', knn_acc)\n",
        "print('Logistic Regression Classifier accuracy:', lr_acc)\n",
        "#print('adab Classifier accuracy:', adab_acc)\n",
        "print('lda Classifier accuracy:', lda_acc)\n",
        "print('qda Classifier accuracy:', qda_acc)\n",
        "#print('xgb Classifier accuracy:', xgb_acc)\n",
        "print('clf Classifier accuracy:', clf_acc)\n",
        "print('ensemble Classifier accuracy:', ensemble_acc)\n",
        "print('ensemble2 Classifier accuracy:', ensemble_acc2)\n",
        "print('ensemble3 Classifier accuracy:', ensemble_acc3)"
      ],
      "metadata": {
        "id": "2s6XSn8vo8Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_samples: maximum size 0.5=50% of each sample taken from the full dataset\n",
        "# max_features: maximum of features 1=100% taken here all 10K \n",
        "# n_estimators: number of decision trees \n",
        "bg=BaggingClassifier(DecisionTreeClassifier(),max_samples=0.5,max_features=1.0,n_estimators=10)\n",
        "bg.fit(X_train31, y_train31)\n",
        "bg_pred = bg.predict(X_test31)\n",
        "bg_acc = accuracy_score(y_test31, bg_pred)\n",
        "print('bg Classifier accuracy:', bg_acc)"
      ],
      "metadata": {
        "id": "jWOLbAeiqJX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adb = AdaBoostClassifier(DecisionTreeClassifier(min_samples_split=10,max_depth=4),n_estimators=10,learning_rate=0.6)\n",
        "adb.fit(X_train31, y_train31)\n",
        "adb_pred = adb.predict(X_test31)\n",
        "adb_acc = accuracy_score(y_test31, adb_pred)\n",
        "print('adb Classifier accuracy:', adb_acc)"
      ],
      "metadata": {
        "id": "uNJEKUDhqQ3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(max_depth=10, n_jobs=-1, random_state=0)\n",
        "rf.fit(X_train31, y_train31)\n",
        "rf_pred = rf.predict(X_test31)\n",
        "rf_acc = accuracy_score(y_test31, rf_pred)\n",
        "print('rf Classifier accuracy:', rf_acc)"
      ],
      "metadata": {
        "id": "9UxvNGwgqmqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Gaussian Process Classifier with RBF kernel\n",
        "kernel = 1.0 * RBF(length_scale=1.0)\n",
        "clf = GaussianProcessClassifier(kernel=kernel, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train31, y_train31)\n",
        "\n",
        "# Predict the class labels for the test data\n",
        "y_pred = clf.predict(X_test31)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test31, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "A_LPFtuPyI-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an SGD Classifier with logistic loss function\n",
        "clf = SGDClassifier(loss='log', penalty='l2', max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train31, y_train31)\n",
        "\n",
        "# Predict the class labels for the test data\n",
        "y_pred = clf.predict(X_test31)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test31, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "fca4zKN_yOwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an SGD Classifier\n",
        "clf = SGDClassifier(random_state=42)\n",
        "\n",
        "# Define a grid of hyperparameters to search over\n",
        "param_grid = {'loss': ['log', 'hinge'], \n",
        "              'penalty': ['l1', 'l2', 'elasticnet'], \n",
        "              'alpha': [0.0001, 0.001, 0.01], \n",
        "              'max_iter': [1000, 2000, 3000], \n",
        "              'learning_rate': ['constant', 'optimal', 'invscaling']}\n",
        "\n",
        "# Use grid search to find the best set of hyperparameters\n",
        "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, verbose=2)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# Print the best set of hyperparameters and the corresponding accuracy score\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "y_pred = grid_search.predict(X_test31)\n",
        "accuracy = accuracy_score(y_test31, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "FiixeaNXzaYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Multinomial Logistic Regression classifier\n",
        "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
        "\n",
        "# Train the classifier on the training set\n",
        "lr.fit(X_train31, y_train31)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = lr.predict(X_test31)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test31, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "iwtglm3002bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Obtain the result on test data using the selected model**"
      ],
      "metadata": {
        "id": "0ZBAdS1RQLTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Only present the final model result here\n",
        "clf2 =  LogisticRegression(C= 1, penalty= 'l2', solver='liblinear')\n",
        "clf2.fit(xtrain, ytrain)\n",
        "# Create our predictions\n",
        "prediction2 = clf2.predict(xtest2)\n",
        "prediction2\n",
        "savetxt('lrgridsubmit.csv', prediction2, delimiter=',',fmt='%s')"
      ],
      "metadata": {
        "id": "xSK1zP8n0Ujd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-class Classification**  \n",
        "Classify walking(1), walking_upstairs(2), walking_downstairs(3), sitting(4), standing(5), lying(6) and static postural transition(7)  \n",
        "\n",
        "The following models are included:  \n",
        "\n",
        "(1) Support Vector Machine with kenel: linear, poly, rbf, sigmoid. \n",
        "\n",
        "(2) Decision Tree \n",
        "\n",
        "(3) Guassian Naive Bayes \n",
        "\n",
        "(4) K-Nearest Neighbors \n",
        "\n",
        "(5) Logistic Regression \n",
        "\n",
        "(6) Linear Discriminant Analysis \n",
        "\n",
        "(7) Quadratic Discriminant Analysis  \n",
        "\n",
        "(8) Random Forest  \n",
        "\n",
        "(9) Voting Classifier \n",
        "\n",
        "(10) Neutral Network \n",
        "\n",
        "(11) Bagging Classifier  \n",
        "\n",
        "(12) AdaBoost Classifier \n",
        "\n",
        "(13) Gaussian Process Classifier with RBF kernel  \n",
        "\n",
        "(14) SGD Classifier \n",
        "\n",
        "(15) Multinomial Logistic Regression"
      ],
      "metadata": {
        "id": "hvOjtH40Q3Rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Enable Data**"
      ],
      "metadata": {
        "id": "TkxlWrVvRUDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2=dataset\n",
        "List3=['7','8','9','10','11','12']\n",
        "dataset2[\"V0\"] = np.where(dataset2['V2'].isin(List3),'7',dataset2['V2'])\n",
        "\n",
        "ytrain2=dataset2[\"V0\"]\n",
        "ytrain2=ytrain2[1:]\n",
        "xtrain2=dataset2.iloc[1:,3:564]\n",
        "xtrain2"
      ],
      "metadata": {
        "id": "FECkW-oLoFM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Split data**"
      ],
      "metadata": {
        "id": "hN6_6tCTRddk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I tried several ways to split the data, including 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4. \n",
        "#Considering the limited space here, this version of code only including the 0.4, \n",
        "#which meeans divide the data into train(60%) and test(40%) data. \n",
        "#You can easily change the fraction as you want.\n",
        "X_train32, X_test32, y_train32, y_test32 = train_test_split(xtrain2, ytrain2 , test_size=0.4, random_state=0)"
      ],
      "metadata": {
        "id": "0AaGTeyv1qjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Build base model**"
      ],
      "metadata": {
        "id": "oVo929_jRvln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Estimator\n",
        "dummy_clf2 = DummyClassifier(strategy='stratified')\n",
        "dummy_clf2.fit(xtrain2,ytrain2)\n",
        "\n",
        "# Check for Model Accuracy\n",
        "dummy_clf2.score(xtrain2,ytrain2)"
      ],
      "metadata": {
        "id": "sbJMBNHYoSxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Obtain optimal parameters**"
      ],
      "metadata": {
        "id": "umLtjM3mR1SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create decision tree classifier\n",
        "dtc = DecisionTreeClassifier()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 4, 6, 8, 10],\n",
        "    'min_samples_split': [2, 4, 6, 8],\n",
        "    'min_samples_leaf': [1, 2, 3, 4]\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(dtc, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "C_E0jR5GpPog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for random_state\n",
        "param_grid = {'random_state': range(0, 101)}\n",
        "\n",
        "# Define the SVM classifier\n",
        "svc = SVC()\n",
        "\n",
        "# Define the grid search object\n",
        "grid_search = GridSearchCV(svc, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search object to the data\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# Print the best random_state parameter value and the corresponding mean test score\n",
        "print('Best random_state:', grid_search.best_params_['random_state'])\n",
        "print('Mean test score:', grid_search.best_score_)"
      ],
      "metadata": {
        "id": "DLFqnW5opXS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "01qg62hCqBRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create LDA classifier\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'solver': ['svd', 'lsqr'],\n",
        "    'shrinkage': [None, 'auto', 0.1, 0.5, 0.9],\n",
        "    'n_components': [1, 2, 3]\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(lda, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "yrX_rkwkppE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create QDA classifier\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'reg_param': [0.0, 0.1, 0.5, 0.9]\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(qda, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "qQ-px60lpvhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(lr, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "-CmEP07ypbGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 20],\n",
        "    'class_weight': [None, 'balanced'],\n",
        "    'n_jobs': [-1, 1, 2],\n",
        "    'random_state': [0,42, 123, 456]\n",
        "}\n",
        "\n",
        "# Create the random forest classifier\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Perform the grid search using cross-validation\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# Print the best hyperparameters and the corresponding validation score\n",
        "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Validation score: {grid_search.best_score_:.2f}\")"
      ],
      "metadata": {
        "id": "PDpz5GHXqzam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameter grid\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100], 'kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
        "\n",
        "# create SVM classifier\n",
        "svc = SVC()\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "3oYtvZwhq_pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Training data evaluation**"
      ],
      "metadata": {
        "id": "T1RNM85dSDQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build multiple classifiers\n",
        "svm0 = LinearSVC()\n",
        "dt = DecisionTreeClassifier( max_depth=4, min_samples_leaf= 2,min_samples_split= 4)\n",
        "nb = GaussianNB()\n",
        "knn = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\n",
        "lr = LogisticRegression(random_state=22)\n",
        "# adab = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100)\n",
        "lda = LinearDiscriminantAnalysis(n_components=1, shrinkage=None, solver='svd')\n",
        "qda = QuadraticDiscriminantAnalysis(reg_param= 0.1)\n",
        "#xgb = xgb.XGBClassifier()\n",
        "svm2 = SVC(random_state=0)\n",
        "clf = RandomForestClassifier(max_depth=10, n_jobs=-1, random_state=0)\n",
        "svm3 = SVC(C=0.1,gamma=0.1,kernel='linear')\n",
        "svm4 = SVC(kernel='poly')\n",
        "svm5 = SVC(kernel='rbf')\n",
        "svm6 = SVC(kernel='sigmoid')\n",
        "# Define the ensemble classifier\n",
        "ensemble = VotingClassifier(estimators=[('dt', svm0), ('lr', svm3), ('svm', svm4)], voting='hard')\n",
        "\n",
        "\n",
        "# Train the classifiers on the training set\n",
        "svm0.fit(X_train32, y_train32)\n",
        "svm3.fit(X_train32, y_train32)\n",
        "svm4.fit(X_train32, y_train32)\n",
        "svm5.fit(X_train32, y_train32)\n",
        "svm6.fit(X_train32, y_train32)\n",
        "dt.fit(X_train32, y_train32)\n",
        "nb.fit(X_train32, y_train32)\n",
        "knn.fit(X_train32, y_train32)\n",
        "lr.fit(X_train32, y_train32)\n",
        "#adab.fit(X_train32, y_train32)\n",
        "lda.fit(X_train32, y_train32)\n",
        "qda.fit(X_train32, y_train32)\n",
        "#xgb.fit(X_train31, y_train31)\n",
        "clf.fit(X_train32, y_train32)\n",
        "ensemble.fit(X_train32, y_train32)\n",
        "\n",
        "# Predict the test set using the trained classifiers\n",
        "svm0_pred = svm0.predict(X_test32)\n",
        "svm3_pred = svm3.predict(X_test32)\n",
        "svm4_pred = svm4.predict(X_test32)\n",
        "svm5_pred = svm5.predict(X_test32)\n",
        "svm6_pred = svm6.predict(X_test32)\n",
        "\n",
        "dt_pred = dt.predict(X_test32)\n",
        "nb_pred = nb.predict(X_test32)\n",
        "knn_pred = knn.predict(X_test32)\n",
        "lr_pred = lr.predict(X_test32)\n",
        "#adab_pred = adab.predict(X_test32)\n",
        "lda_pred = lda.predict(X_test32)\n",
        "qda_pred = qda.predict(X_test32)\n",
        "#xgb_pred = xgb.predict(X_test31)\n",
        "clf_pred = clf.predict(X_test32)\n",
        "ensemble_pred = ensemble.predict(X_test32)\n",
        "\n",
        "# Evaluate the accuracy of each classifier on the test set\n",
        "svm0_acc = accuracy_score(y_test32, svm0_pred)\n",
        "svm3_acc = accuracy_score(y_test32, svm3_pred)\n",
        "svm4_acc = accuracy_score(y_test32, svm4_pred)\n",
        "svm5_acc = accuracy_score(y_test32, svm5_pred)\n",
        "svm6_acc = accuracy_score(y_test32, svm6_pred)\n",
        "\n",
        "dt_acc = accuracy_score(y_test32, dt_pred)\n",
        "nb_acc = accuracy_score(y_test32, nb_pred)\n",
        "knn_acc = accuracy_score(y_test32, knn_pred)\n",
        "lr_acc = accuracy_score(y_test32, lr_pred)\n",
        "#adab_acc = accuracy_score(y_test32, adab_pred)\n",
        "lda_acc = accuracy_score(y_test32, lda_pred)\n",
        "qda_acc = accuracy_score(y_test32, qda_pred)\n",
        "#xgb_acc = accuracy_score(y_test31, xgb_pred)\n",
        "clf_acc = accuracy_score(y_test32, clf_pred)\n",
        "ensemble_acc = accuracy_score(y_test32, ensemble_pred)\n",
        "\n",
        "print('svm0 Classifier accuracy:', svm0_acc)\n",
        "print('svm3 Classifier accuracy:', svm3_acc)\n",
        "print('svm4 Classifier accuracy:', svm4_acc)\n",
        "print('svm5 Classifier accuracy:', svm5_acc)\n",
        "print('svm6 Classifier accuracy:', svm6_acc)\n",
        "\n",
        "print('dt Classifier accuracy:', dt_acc)\n",
        "print('nb Classifier accuracy:', nb_acc)\n",
        "print('K-Nearest Neighbors Classifier accuracy:', knn_acc)\n",
        "print('Logistic Regression Classifier accuracy:', lr_acc)\n",
        "#print('adab Classifier accuracy:', adab_acc)\n",
        "print('lda Classifier accuracy:', lda_acc)\n",
        "print('qda Classifier accuracy:', qda_acc)\n",
        "#print('xgb Classifier accuracy:', xgb_acc)\n",
        "print('clf Classifier accuracy:', clf_acc)\n",
        "print('ensemble Classifier accuracy:', ensemble_acc)"
      ],
      "metadata": {
        "id": "0VbSeRpfoeTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_samples: maximum size 0.5=50% of each sample taken from the full dataset\n",
        "# max_features: maximum of features 1=100% taken here all 10K \n",
        "# n_estimators: number of decision trees \n",
        "bg=BaggingClassifier(DecisionTreeClassifier(),max_samples=0.5,max_features=1.0,n_estimators=10)\n",
        "bg.fit(X_train32, y_train32)\n",
        "bg_pred = bg.predict(X_test32)\n",
        "bg_acc = accuracy_score(y_test32, bg_pred)\n",
        "print('bg Classifier accuracy:', bg_acc)"
      ],
      "metadata": {
        "id": "jCzvz7e4ovli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adb = AdaBoostClassifier(DecisionTreeClassifier(min_samples_split=10,max_depth=4),n_estimators=10,learning_rate=0.6)\n",
        "adb.fit(X_train32, y_train32)\n",
        "adb_pred = adb.predict(X_test32)\n",
        "adb_acc = accuracy_score(y_test32, adb_pred)\n",
        "print('adb Classifier accuracy:', adb_acc)"
      ],
      "metadata": {
        "id": "UN2Oj18GqWCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(max_depth=10, n_jobs=-1, random_state=0)\n",
        "rf.fit(X_train32, y_train32)\n",
        "rf_pred = rf.predict(X_test32)\n",
        "rf_acc = accuracy_score(y_test32, rf_pred)\n",
        "print('rf Classifier accuracy:', rf_acc)"
      ],
      "metadata": {
        "id": "kf3oIaegqfel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Gaussian Process Classifier with RBF kernel\n",
        "kernel = 1.0 * RBF(length_scale=1.0)\n",
        "clf = GaussianProcessClassifier(kernel=kernel, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train32, y_train32)\n",
        "\n",
        "# Predict the class labels for the test data\n",
        "y_pred = clf.predict(X_test32)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test32, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "oXrNWohuyCXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an SGD Classifier with logistic loss function\n",
        "clf = SGDClassifier(loss='log', penalty='l2', max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train32, y_train32)\n",
        "\n",
        "# Predict the class labels for the test data\n",
        "y_pred = clf.predict(X_test32)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test32, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "ghMP8DCGyTMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an SGD Classifier\n",
        "clf = SGDClassifier(random_state=42)\n",
        "\n",
        "# Define a grid of hyperparameters to search over\n",
        "param_grid = {'loss': ['log', 'hinge'], \n",
        "              'penalty': ['l1', 'l2', 'elasticnet'], \n",
        "              'alpha': [0.0001, 0.001, 0.01], \n",
        "              'max_iter': [1000, 2000, 3000], \n",
        "              'learning_rate': ['constant', 'optimal', 'invscaling']}\n",
        "\n",
        "# Use grid search to find the best set of hyperparameters\n",
        "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, verbose=2)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# Print the best set of hyperparameters and the corresponding accuracy score\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "y_pred = grid_search.predict(X_test32)\n",
        "accuracy = accuracy_score(y_test32, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "wqLNDepFzViL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Multinomial Logistic Regression classifier\n",
        "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
        "\n",
        "# Train the classifier on the training set\n",
        "lr.fit(X_train32, y_train32)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = lr.predict(X_test32)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test32, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "vX_IwTde0sZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data for network model"
      ],
      "metadata": {
        "id": "1yKFP498W-dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain = xtrain.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# print the converted DataFrame\n",
        "print(xtrain)\n",
        "xtrain.dtypes"
      ],
      "metadata": {
        "id": "hGsNb-9OwVqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtest2 = xtest2.apply(pd.to_numeric, errors='coerce')\n",
        "print(xtest2)\n",
        "xtest2.dtypes"
      ],
      "metadata": {
        "id": "WKKUHw90wV0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytrain = ytrain.apply(pd.to_numeric, errors='coerce')\n",
        "print(ytrain)\n",
        "ytrain.dtypes"
      ],
      "metadata": {
        "id": "kAQ4s7MNwc-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train32, X_test32, y_train32, y_test32 = train_test_split(xtrain, ytrain , test_size=0.4, random_state=0)"
      ],
      "metadata": {
        "id": "wOf19_2u1hH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train32 = X_train32.apply(pd.to_numeric)\n",
        "\n",
        "# convert dataframe to numpy array\n",
        "my_array = np.array(X_train32)\n",
        "\n",
        "print(my_array)"
      ],
      "metadata": {
        "id": "34u3w9XXwdvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test32 = X_test32.apply(pd.to_numeric)\n",
        "\n",
        "# convert dataframe to numpy array\n",
        "my_arrayyt = np.array(X_test32)\n",
        "\n",
        "print(my_arrayyt)"
      ],
      "metadata": {
        "id": "e3r6hj-Wwe4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# convert object dtype columns to numeric\n",
        "y_train32 = y_train32.apply(pd.to_numeric)\n",
        "\n",
        "# convert dataframe to numpy array\n",
        "my_arrayy = np.array(y_train32)\n",
        "\n",
        "print(my_arrayy)"
      ],
      "metadata": {
        "id": "aDOVFpwXwe75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test32 = y_test32.apply(pd.to_numeric)\n",
        "\n",
        "# convert dataframe to numpy array\n",
        "my_arrayytt = np.array(y_test32)\n",
        "\n",
        "print(my_arrayytt)"
      ],
      "metadata": {
        "id": "E02ggDZvwjGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sample dataset\n",
        "X = my_array\n",
        "y = my_arrayy\n",
        "Xt = my_arrayyt\n",
        "yt = my_arrayytt\n",
        "# define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=561, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model.fit(X, y, epochs=100, batch_size=10)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(Xt, yt)\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "# make predictions\n",
        "predictions = model.predict(Xt)\n",
        "print(\"Predictions: \", predictions)"
      ],
      "metadata": {
        "id": "mFrHiEAmwWOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Obtain result on test data**"
      ],
      "metadata": {
        "id": "Q50vuv-5XIjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Only show the final chosen model here\n",
        "ensemble = SVC(C=1,kernel='linear')\n",
        "ensemble.fit(xtrain2, ytrain2)\n",
        "# make a single prediction\n",
        "ensemble.score(xtrain2,ytrain2)\n",
        "yhat2 = ensemble.predict(xtest2)\n",
        "yhat2\n",
        "savetxt('linearsvmc1.csv', yhat2, delimiter=',',fmt='%s')"
      ],
      "metadata": {
        "id": "AVddmE7A1LOC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}