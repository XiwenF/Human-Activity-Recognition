{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-wePfkc-bwY"
      },
      "source": [
        "**Import** **Module** **and data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "9T2oRaTPnim8",
        "outputId": "b8233bad-c08b-4269-e430-d3ab653d61a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0       V1        V2           V3            V4            V5  \\\n",
              "0           1  subject  activity           F1            F2            F3   \n",
              "1           2        1         5  0.043579674  -0.005970221  -0.035054344   \n",
              "2           3        1         5  0.039480037  -0.002131276  -0.029067362   \n",
              "3           4        1         5  0.039977781  -0.005152716  -0.022650708   \n",
              "4           5        1         5  0.039784558  -0.011808778  -0.028915779   \n",
              "\n",
              "             V6            V7            V8            V9  ...          V554  \\\n",
              "0            F4            F5            F6            F7  ...          F552   \n",
              "1   -0.99538116  -0.988365863  -0.937382005  -0.995007045  ...  -0.012235894   \n",
              "2  -0.998347999  -0.982944945  -0.971272882  -0.998701962  ...   0.202803809   \n",
              "3  -0.995482127  -0.977313843  -0.984759524  -0.996414838  ...   0.440079356   \n",
              "4    -0.9961941  -0.988568594  -0.993255602  -0.996994335  ...    0.43089077   \n",
              "\n",
              "           V555          V556          V557          V558          V559  \\\n",
              "0          F553          F554          F555          F556          F557   \n",
              "1  -0.314848353  -0.713307812  -0.112754341   0.030400372  -0.464761386   \n",
              "2  -0.603199224  -0.860676901   0.053476955  -0.007434566   -0.73262621   \n",
              "3   -0.40442749  -0.761847228  -0.118559255   0.177899475   0.100699208   \n",
              "4   -0.13837282  -0.491604347  -0.036787973  -0.012892494   0.640011043   \n",
              "\n",
              "           V560          V561         V562          V563  \n",
              "0          F558          F559         F560          F561  \n",
              "1  -0.018445884  -0.841558511  0.179912811  -0.051718416  \n",
              "2   0.703510588  -0.845092399   0.18026111  -0.047436337  \n",
              "3   0.808529075  -0.849230131  0.180609558  -0.042271363  \n",
              "4  -0.485366445  -0.848946592  0.181907092  -0.040826223  \n",
              "\n",
              "[5 rows x 564 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e655349-0687-46e9-9f2d-2f3757326feb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V554</th>\n",
              "      <th>V555</th>\n",
              "      <th>V556</th>\n",
              "      <th>V557</th>\n",
              "      <th>V558</th>\n",
              "      <th>V559</th>\n",
              "      <th>V560</th>\n",
              "      <th>V561</th>\n",
              "      <th>V562</th>\n",
              "      <th>V563</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>subject</td>\n",
              "      <td>activity</td>\n",
              "      <td>F1</td>\n",
              "      <td>F2</td>\n",
              "      <td>F3</td>\n",
              "      <td>F4</td>\n",
              "      <td>F5</td>\n",
              "      <td>F6</td>\n",
              "      <td>F7</td>\n",
              "      <td>...</td>\n",
              "      <td>F552</td>\n",
              "      <td>F553</td>\n",
              "      <td>F554</td>\n",
              "      <td>F555</td>\n",
              "      <td>F556</td>\n",
              "      <td>F557</td>\n",
              "      <td>F558</td>\n",
              "      <td>F559</td>\n",
              "      <td>F560</td>\n",
              "      <td>F561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.043579674</td>\n",
              "      <td>-0.005970221</td>\n",
              "      <td>-0.035054344</td>\n",
              "      <td>-0.99538116</td>\n",
              "      <td>-0.988365863</td>\n",
              "      <td>-0.937382005</td>\n",
              "      <td>-0.995007045</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.012235894</td>\n",
              "      <td>-0.314848353</td>\n",
              "      <td>-0.713307812</td>\n",
              "      <td>-0.112754341</td>\n",
              "      <td>0.030400372</td>\n",
              "      <td>-0.464761386</td>\n",
              "      <td>-0.018445884</td>\n",
              "      <td>-0.841558511</td>\n",
              "      <td>0.179912811</td>\n",
              "      <td>-0.051718416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.039480037</td>\n",
              "      <td>-0.002131276</td>\n",
              "      <td>-0.029067362</td>\n",
              "      <td>-0.998347999</td>\n",
              "      <td>-0.982944945</td>\n",
              "      <td>-0.971272882</td>\n",
              "      <td>-0.998701962</td>\n",
              "      <td>...</td>\n",
              "      <td>0.202803809</td>\n",
              "      <td>-0.603199224</td>\n",
              "      <td>-0.860676901</td>\n",
              "      <td>0.053476955</td>\n",
              "      <td>-0.007434566</td>\n",
              "      <td>-0.73262621</td>\n",
              "      <td>0.703510588</td>\n",
              "      <td>-0.845092399</td>\n",
              "      <td>0.18026111</td>\n",
              "      <td>-0.047436337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.039977781</td>\n",
              "      <td>-0.005152716</td>\n",
              "      <td>-0.022650708</td>\n",
              "      <td>-0.995482127</td>\n",
              "      <td>-0.977313843</td>\n",
              "      <td>-0.984759524</td>\n",
              "      <td>-0.996414838</td>\n",
              "      <td>...</td>\n",
              "      <td>0.440079356</td>\n",
              "      <td>-0.40442749</td>\n",
              "      <td>-0.761847228</td>\n",
              "      <td>-0.118559255</td>\n",
              "      <td>0.177899475</td>\n",
              "      <td>0.100699208</td>\n",
              "      <td>0.808529075</td>\n",
              "      <td>-0.849230131</td>\n",
              "      <td>0.180609558</td>\n",
              "      <td>-0.042271363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.039784558</td>\n",
              "      <td>-0.011808778</td>\n",
              "      <td>-0.028915779</td>\n",
              "      <td>-0.9961941</td>\n",
              "      <td>-0.988568594</td>\n",
              "      <td>-0.993255602</td>\n",
              "      <td>-0.996994335</td>\n",
              "      <td>...</td>\n",
              "      <td>0.43089077</td>\n",
              "      <td>-0.13837282</td>\n",
              "      <td>-0.491604347</td>\n",
              "      <td>-0.036787973</td>\n",
              "      <td>-0.012892494</td>\n",
              "      <td>0.640011043</td>\n",
              "      <td>-0.485366445</td>\n",
              "      <td>-0.848946592</td>\n",
              "      <td>0.181907092</td>\n",
              "      <td>-0.040826223</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 564 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e655349-0687-46e9-9f2d-2f3757326feb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6e655349-0687-46e9-9f2d-2f3757326feb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6e655349-0687-46e9-9f2d-2f3757326feb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from numpy import savetxt\n",
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Support Vector Machines\n",
        "from sklearn.svm import LinearSVC\n",
        "# Decision Trees\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "# K-Nearest Neighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from matplotlib import pyplot\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import xgboost as xgb\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from   sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import  KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "dataset = pd.read_csv('training.csv',engine='python' )\n",
        "dataset_test = pd.read_csv('test.csv',engine='python' )\n",
        "dataset.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z3_TlngGzQc"
      },
      "source": [
        "**Binary Classification** \n",
        "\n",
        "Classify the activity into static(0) and dynamic(1)\n",
        "\n",
        "The following models are included:  \n",
        "\n",
        "(1) Support Vector Machine with kenel: linear, poly, rbf, sigmoid. \n",
        "\n",
        "(2) Decision Tree \n",
        "\n",
        "(3) Guassian Naive Bayes \n",
        "\n",
        "(4) K-Nearest Neighbors \n",
        "\n",
        "(5) Logistic Regression \n",
        "\n",
        "(6) Linear Discriminant Analysis \n",
        "\n",
        "(7) Quadratic Discriminant Analysis  \n",
        "\n",
        "(8) Random Forest  \n",
        "\n",
        "(9) Voting Classifier \n",
        "\n",
        "(10) Neutral Network \n",
        "\n",
        "(11) Bagging Classifier  \n",
        "\n",
        "(12) AdaBoost Classifier \n",
        "\n",
        "(13) Gaussian Process Classifier with RBF kernel  \n",
        "\n",
        "(14) SGD Classifier \n",
        "\n",
        "(15) Multinomial Logistic Regression\n",
        "\n",
        "(16) Gradient Boost Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYqvbr9XHTlB"
      },
      "source": [
        "**Step 1: Enable Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "m00jyX8GnlFS",
        "outputId": "edb86323-73bc-4328-9867-22b4910483bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0       V1        V2           V3            V4            V5  \\\n",
              "0              1  subject  activity           F1            F2            F3   \n",
              "1              2        1         5  0.043579674  -0.005970221  -0.035054344   \n",
              "2              3        1         5  0.039480037  -0.002131276  -0.029067362   \n",
              "3              4        1         5  0.039977781  -0.005152716  -0.022650708   \n",
              "4              5        1         5  0.039784558  -0.011808778  -0.028915779   \n",
              "...          ...      ...       ...          ...           ...           ...   \n",
              "7763        7764       30         2  0.048048374  -0.042445161  -0.065884336   \n",
              "7764        7765       30         2  0.037638604   0.006430371  -0.044344723   \n",
              "7765        7766       30         2  0.037450938  -0.002724424   0.021009408   \n",
              "7766        7767       30         2  0.044011045  -0.004535781  -0.051242204   \n",
              "7767        7768       30         2  0.068953765   0.001810322  -0.080323431   \n",
              "\n",
              "                V6            V7            V8            V9  ...  \\\n",
              "0               F4            F5            F6            F7  ...   \n",
              "1      -0.99538116  -0.988365863  -0.937382005  -0.995007045  ...   \n",
              "2     -0.998347999  -0.982944945  -0.971272882  -0.998701962  ...   \n",
              "3     -0.995482127  -0.977313843  -0.984759524  -0.996414838  ...   \n",
              "4       -0.9961941  -0.988568594  -0.993255602  -0.996994335  ...   \n",
              "...            ...           ...           ...           ...  ...   \n",
              "7763  -0.195447967  -0.278326363  -0.219953526  -0.282233132  ...   \n",
              "7764  -0.235372031   -0.30268012  -0.232843488  -0.322482719  ...   \n",
              "7765    -0.2182808   -0.37808216  -0.076950379  -0.304446475  ...   \n",
              "7766  -0.219202106   -0.38334992   -0.08103469  -0.310418503  ...   \n",
              "7767  -0.269335686  -0.366553435  -0.147294234  -0.377331537  ...   \n",
              "\n",
              "              V555          V556          V557          V558          V559  \\\n",
              "0             F553          F554          F555          F556          F557   \n",
              "1     -0.314848353  -0.713307812  -0.112754341   0.030400372  -0.464761386   \n",
              "2     -0.603199224  -0.860676901   0.053476955  -0.007434566   -0.73262621   \n",
              "3      -0.40442749  -0.761847228  -0.118559255   0.177899475   0.100699208   \n",
              "4      -0.13837282  -0.491604347  -0.036787973  -0.012892494   0.640011043   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "7763  -0.596760198  -0.879025634  -0.190436861   0.829718416   0.206972154   \n",
              "7764  -0.404417928  -0.684496226   0.064906712   0.875679049  -0.879032789   \n",
              "7765   0.000206574  -0.317314291   0.052805928  -0.266724365   0.864404011   \n",
              "7766   0.037918739  -0.356579108  -0.101360118   0.700739689   0.936673944   \n",
              "7767   -0.40083141  -0.742971769  -0.280088053  -0.007739278  -0.056087594   \n",
              "\n",
              "              V560          V561         V562          V563 V0  \n",
              "0             F558          F559         F560          F561  0  \n",
              "1     -0.018445884  -0.841558511  0.179912811  -0.051718416  0  \n",
              "2      0.703510588  -0.845092399   0.18026111  -0.047436337  0  \n",
              "3      0.808529075  -0.849230131  0.180609558  -0.042271363  0  \n",
              "4     -0.485366445  -0.848946592  0.181907092  -0.040826223  0  \n",
              "...            ...           ...          ...           ... ..  \n",
              "7763  -0.425618579  -0.792291739  0.238580336   0.056019937  1  \n",
              "7764    0.40021936  -0.772287652  0.252652789   0.056251833  1  \n",
              "7765   0.701168816  -0.779566335  0.249121453   0.047070771  1  \n",
              "7766   -0.58947895   -0.78560327  0.246408672   0.031700029  1  \n",
              "7767   -0.61695645  -0.783692535  0.246784989   0.042981289  1  \n",
              "\n",
              "[7768 rows x 565 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c4ec204-5498-49d6-80d6-573a8e7684b9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V555</th>\n",
              "      <th>V556</th>\n",
              "      <th>V557</th>\n",
              "      <th>V558</th>\n",
              "      <th>V559</th>\n",
              "      <th>V560</th>\n",
              "      <th>V561</th>\n",
              "      <th>V562</th>\n",
              "      <th>V563</th>\n",
              "      <th>V0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>subject</td>\n",
              "      <td>activity</td>\n",
              "      <td>F1</td>\n",
              "      <td>F2</td>\n",
              "      <td>F3</td>\n",
              "      <td>F4</td>\n",
              "      <td>F5</td>\n",
              "      <td>F6</td>\n",
              "      <td>F7</td>\n",
              "      <td>...</td>\n",
              "      <td>F553</td>\n",
              "      <td>F554</td>\n",
              "      <td>F555</td>\n",
              "      <td>F556</td>\n",
              "      <td>F557</td>\n",
              "      <td>F558</td>\n",
              "      <td>F559</td>\n",
              "      <td>F560</td>\n",
              "      <td>F561</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.043579674</td>\n",
              "      <td>-0.005970221</td>\n",
              "      <td>-0.035054344</td>\n",
              "      <td>-0.99538116</td>\n",
              "      <td>-0.988365863</td>\n",
              "      <td>-0.937382005</td>\n",
              "      <td>-0.995007045</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.314848353</td>\n",
              "      <td>-0.713307812</td>\n",
              "      <td>-0.112754341</td>\n",
              "      <td>0.030400372</td>\n",
              "      <td>-0.464761386</td>\n",
              "      <td>-0.018445884</td>\n",
              "      <td>-0.841558511</td>\n",
              "      <td>0.179912811</td>\n",
              "      <td>-0.051718416</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.039480037</td>\n",
              "      <td>-0.002131276</td>\n",
              "      <td>-0.029067362</td>\n",
              "      <td>-0.998347999</td>\n",
              "      <td>-0.982944945</td>\n",
              "      <td>-0.971272882</td>\n",
              "      <td>-0.998701962</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.603199224</td>\n",
              "      <td>-0.860676901</td>\n",
              "      <td>0.053476955</td>\n",
              "      <td>-0.007434566</td>\n",
              "      <td>-0.73262621</td>\n",
              "      <td>0.703510588</td>\n",
              "      <td>-0.845092399</td>\n",
              "      <td>0.18026111</td>\n",
              "      <td>-0.047436337</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.039977781</td>\n",
              "      <td>-0.005152716</td>\n",
              "      <td>-0.022650708</td>\n",
              "      <td>-0.995482127</td>\n",
              "      <td>-0.977313843</td>\n",
              "      <td>-0.984759524</td>\n",
              "      <td>-0.996414838</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.40442749</td>\n",
              "      <td>-0.761847228</td>\n",
              "      <td>-0.118559255</td>\n",
              "      <td>0.177899475</td>\n",
              "      <td>0.100699208</td>\n",
              "      <td>0.808529075</td>\n",
              "      <td>-0.849230131</td>\n",
              "      <td>0.180609558</td>\n",
              "      <td>-0.042271363</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.039784558</td>\n",
              "      <td>-0.011808778</td>\n",
              "      <td>-0.028915779</td>\n",
              "      <td>-0.9961941</td>\n",
              "      <td>-0.988568594</td>\n",
              "      <td>-0.993255602</td>\n",
              "      <td>-0.996994335</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.13837282</td>\n",
              "      <td>-0.491604347</td>\n",
              "      <td>-0.036787973</td>\n",
              "      <td>-0.012892494</td>\n",
              "      <td>0.640011043</td>\n",
              "      <td>-0.485366445</td>\n",
              "      <td>-0.848946592</td>\n",
              "      <td>0.181907092</td>\n",
              "      <td>-0.040826223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7763</th>\n",
              "      <td>7764</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>0.048048374</td>\n",
              "      <td>-0.042445161</td>\n",
              "      <td>-0.065884336</td>\n",
              "      <td>-0.195447967</td>\n",
              "      <td>-0.278326363</td>\n",
              "      <td>-0.219953526</td>\n",
              "      <td>-0.282233132</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.596760198</td>\n",
              "      <td>-0.879025634</td>\n",
              "      <td>-0.190436861</td>\n",
              "      <td>0.829718416</td>\n",
              "      <td>0.206972154</td>\n",
              "      <td>-0.425618579</td>\n",
              "      <td>-0.792291739</td>\n",
              "      <td>0.238580336</td>\n",
              "      <td>0.056019937</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7764</th>\n",
              "      <td>7765</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>0.037638604</td>\n",
              "      <td>0.006430371</td>\n",
              "      <td>-0.044344723</td>\n",
              "      <td>-0.235372031</td>\n",
              "      <td>-0.30268012</td>\n",
              "      <td>-0.232843488</td>\n",
              "      <td>-0.322482719</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.404417928</td>\n",
              "      <td>-0.684496226</td>\n",
              "      <td>0.064906712</td>\n",
              "      <td>0.875679049</td>\n",
              "      <td>-0.879032789</td>\n",
              "      <td>0.40021936</td>\n",
              "      <td>-0.772287652</td>\n",
              "      <td>0.252652789</td>\n",
              "      <td>0.056251833</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7765</th>\n",
              "      <td>7766</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>0.037450938</td>\n",
              "      <td>-0.002724424</td>\n",
              "      <td>0.021009408</td>\n",
              "      <td>-0.2182808</td>\n",
              "      <td>-0.37808216</td>\n",
              "      <td>-0.076950379</td>\n",
              "      <td>-0.304446475</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000206574</td>\n",
              "      <td>-0.317314291</td>\n",
              "      <td>0.052805928</td>\n",
              "      <td>-0.266724365</td>\n",
              "      <td>0.864404011</td>\n",
              "      <td>0.701168816</td>\n",
              "      <td>-0.779566335</td>\n",
              "      <td>0.249121453</td>\n",
              "      <td>0.047070771</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7766</th>\n",
              "      <td>7767</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>0.044011045</td>\n",
              "      <td>-0.004535781</td>\n",
              "      <td>-0.051242204</td>\n",
              "      <td>-0.219202106</td>\n",
              "      <td>-0.38334992</td>\n",
              "      <td>-0.08103469</td>\n",
              "      <td>-0.310418503</td>\n",
              "      <td>...</td>\n",
              "      <td>0.037918739</td>\n",
              "      <td>-0.356579108</td>\n",
              "      <td>-0.101360118</td>\n",
              "      <td>0.700739689</td>\n",
              "      <td>0.936673944</td>\n",
              "      <td>-0.58947895</td>\n",
              "      <td>-0.78560327</td>\n",
              "      <td>0.246408672</td>\n",
              "      <td>0.031700029</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7767</th>\n",
              "      <td>7768</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>0.068953765</td>\n",
              "      <td>0.001810322</td>\n",
              "      <td>-0.080323431</td>\n",
              "      <td>-0.269335686</td>\n",
              "      <td>-0.366553435</td>\n",
              "      <td>-0.147294234</td>\n",
              "      <td>-0.377331537</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.40083141</td>\n",
              "      <td>-0.742971769</td>\n",
              "      <td>-0.280088053</td>\n",
              "      <td>-0.007739278</td>\n",
              "      <td>-0.056087594</td>\n",
              "      <td>-0.61695645</td>\n",
              "      <td>-0.783692535</td>\n",
              "      <td>0.246784989</td>\n",
              "      <td>0.042981289</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7768 rows × 565 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c4ec204-5498-49d6-80d6-573a8e7684b9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7c4ec204-5498-49d6-80d6-573a8e7684b9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7c4ec204-5498-49d6-80d6-573a8e7684b9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "List0 = ['4','5','6']\n",
        "List1 = ['1','2','3']\n",
        "List2 = ['7','8','9','10','11','12']\n",
        "dataset[\"V0\"] = np.where(dataset['V2'].isin(List1),'1','0')\n",
        "ytrain = dataset[\"V0\"]\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TXlFcIqenpAA"
      },
      "outputs": [],
      "source": [
        "xtrain=dataset.iloc[1:,3:564]\n",
        "xtrain\n",
        "ytrain=ytrain.iloc[1:]\n",
        "dataset_test['V2']=0\n",
        "dataset_test_x = pd.read_csv('xtest.csv',engine='python' )\n",
        "dataset_test_x\n",
        "xtest2=dataset_test_x.iloc[0:,1:]\n",
        "xtest2\n",
        "dataset_test.head()\n",
        "models = {}\n",
        "xtrain=dataset.iloc[1:,3:564]\n",
        "\n",
        "dataset_test_x = pd.read_csv('xtest.csv',engine='python' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAp2dDzhIZZx"
      },
      "source": [
        "**Step 2: Split Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bPl5Vg9vozq8"
      },
      "outputs": [],
      "source": [
        "#I tried several ways to split the data, including 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4. \n",
        "#Considering the limited space here, this version of code only including the 0.4, \n",
        "#which meeans divide the data into train(60%) and test(40%) data. \n",
        "#You can easily change the fraction as you want.\n",
        "X_train31, X_test31, y_train31, y_test31 = train_test_split(xtrain, ytrain , test_size=0.4, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryN3qxfaIAJQ"
      },
      "source": [
        "**Step 3: Build base model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "3Yi4Kcp0oRjo",
        "outputId": "24dbf023-5c64-4791-b719-9e520c30937d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initialize Estimator\n",
        "dummy = DummyClassifier(strategy='stratified')\n",
        "dummy.fit(X_train31,y_train31)\n",
        "\n",
        "# Check for Model Accuracy\n",
        "\n",
        "cm = confusion_matrix(y_test31,dummy.predict(X_test31), labels=dummy.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dummy.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avYVaLYDM-Fq"
      },
      "source": [
        "**Step 4: Find the opimal parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svp2oaO6o2XY",
        "outputId": "3482138c-49ed-417a-ebf7-ac2b181bc5c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best random_state: 0\n",
            "Mean test score: 0.9991416309012877\n"
          ]
        }
      ],
      "source": [
        "# Define the parameter grid for random_state\n",
        "param_grid = {'random_state': range(0, 101)}\n",
        "\n",
        "# Define the SVM classifier\n",
        "svc = SVC()\n",
        "\n",
        "# Define the grid search object\n",
        "grid_search = GridSearchCV(svc, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search object to the data\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# Print the best random_state parameter value and the corresponding mean test score\n",
        "print('Best random_state:', grid_search.best_params_['random_state'])\n",
        "print('Mean test score:', grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayc5CilGpRsU",
        "outputId": "1aeb29af-719c-4b9d-8bd0-c7b5fd27995b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 8}\n"
          ]
        }
      ],
      "source": [
        "# create decision tree classifier\n",
        "dtc = DecisionTreeClassifier()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 4, 6, 8, 10],\n",
        "    'min_samples_split': [2, 4, 6, 8],\n",
        "    'min_samples_leaf': [1, 2, 3, 4]\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(dtc, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kAifWooYpd6v",
        "outputId": "b6b2ef7a-69e8-4d32-9df0-bd9692473877"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n"
          ]
        }
      ],
      "source": [
        "# create logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(lr, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z91-ZqtUpj5A",
        "outputId": "ebce9cd3-f060-4f2f-9fc4-acb14959c49e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "120 fits failed out of a total of 150.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py\", line 615, in fit\n",
            "    raise NotImplementedError(\"shrinkage not supported with 'svd' solver.\")\n",
            "NotImplementedError: shrinkage not supported with 'svd' solver.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "100 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py\", line 608, in fit\n",
            "    raise ValueError(\n",
            "ValueError: n_components cannot be larger than min(n_features, n_classes - 1).\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.99957082 0.50579399        nan 0.99892704        nan 0.99892704\n",
            "        nan 0.99828326        nan 0.99613734        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_components': 1, 'shrinkage': None, 'solver': 'svd'}\n"
          ]
        }
      ],
      "source": [
        "# create LDA classifier\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'solver': ['svd', 'lsqr'],\n",
        "    'shrinkage': [None, 'auto', 0.1, 0.5, 0.9],\n",
        "    'n_components': [1, 2, 3]\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(lda, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MELY4Lbcp-FN",
        "outputId": "d7237f4b-bb1d-423a-f139-8f4750aca584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'metric': 'manhattan', 'n_neighbors': 1, 'weights': 'uniform'}\n"
          ]
        }
      ],
      "source": [
        "# create KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R4W4sX1Fp5Z9",
        "outputId": "5ced43af-44a2-4c67-bd88-f91079ead016"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'reg_param': 0.1}\n"
          ]
        }
      ],
      "source": [
        "# create QDA classifier\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'reg_param': [0.0, 0.1, 0.5, 0.9]\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(qda, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U12fhU7iquQk",
        "outputId": "e82d3ba6-3315-4d58-9322-8f6e181629a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'class_weight': 'balanced', 'max_depth': 10, 'n_jobs': -1, 'random_state': 456}\n",
            "Validation score: 1.00\n"
          ]
        }
      ],
      "source": [
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 20],\n",
        "    'class_weight': [None, 'balanced'],\n",
        "    'n_jobs': [-1, 1, 2],\n",
        "    'random_state': [0,42, 123, 456]\n",
        "}\n",
        "\n",
        "# Create the random forest classifier\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Perform the grid search using cross-validation\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# Print the best hyperparameters and the corresponding validation score\n",
        "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Validation score: {grid_search.best_score_:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jxJrGr1uq6dB",
        "outputId": "67324333-4a9d-4b14-d4de-c994e2b22152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 0.1, 'gamma': 0.1, 'kernel': 'poly'}\n"
          ]
        }
      ],
      "source": [
        "# define parameter grid\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100], 'kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
        "\n",
        "# create SVM classifier\n",
        "svc = SVC()\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ1eFk6uOkuQ"
      },
      "source": [
        "**Step 5: Training data evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2s6XSn8vo8Nj",
        "outputId": "89b39cc0-7dec-43cc-b2a8-c05fc4831f21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "svm0 Classifier accuracy: 0.9996781461216607\n",
            "svm3 Classifier accuracy: 0.9996781461216607\n",
            "svm4 Classifier accuracy: 1.0\n",
            "svm5 Classifier accuracy: 0.9993562922433216\n",
            "svm6 Classifier accuracy: 0.9893788220148053\n",
            "dt Classifier accuracy: 0.9967814612166077\n",
            "nb Classifier accuracy: 0.9874476987447699\n",
            "K-Nearest Neighbors Classifier accuracy: 0.9980688767299646\n",
            "Logistic Regression Classifier accuracy: 1.0\n",
            "lda Classifier accuracy: 0.9996781461216607\n",
            "qda Classifier accuracy: 0.9993562922433216\n",
            "clf Classifier accuracy: 0.9996781461216607\n",
            "ensemble Classifier accuracy: 0.9996781461216607\n",
            "ensemble2 Classifier accuracy: 0.9996781461216607\n",
            "ensemble3 Classifier accuracy: 0.9996781461216607\n"
          ]
        }
      ],
      "source": [
        "# Build multiple classifiers\n",
        "svm0 = LinearSVC()\n",
        "dt = DecisionTreeClassifier( max_depth=4, min_samples_leaf= 2,min_samples_split= 4)\n",
        "nb = GaussianNB()\n",
        "knn = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\n",
        "lr = LogisticRegression(C= 1, penalty= 'l2', solver='liblinear')\n",
        "# adab = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100)\n",
        "lda = LinearDiscriminantAnalysis(n_components=1, shrinkage=None, solver='svd')\n",
        "qda = QuadraticDiscriminantAnalysis(reg_param= 0.1)\n",
        "#xgb = xgb.XGBClassifier()\n",
        "svm2 = SVC(random_state=0)\n",
        "clf = RandomForestClassifier(max_depth=10, n_jobs=-1, random_state=0)\n",
        "svm3 = SVC(C=0.1,gamma=0.1,kernel='linear')\n",
        "svm4 = SVC(kernel='poly')\n",
        "svm5 = SVC(kernel='rbf')\n",
        "svm6 = SVC(kernel='sigmoid')\n",
        "# Define the ensemble classifier\n",
        "ensemble = VotingClassifier(estimators=[('dt', svm0), ('lr', svm3), ('svm', svm4)], voting='hard')\n",
        "# Define the ensemble classifier\n",
        "ensemble2 = VotingClassifier(estimators=[('dt', dt ), ('lr', lr), ('knn', knn ), ('lda', lda), ('qda', qda),('svm6', svm6), ('nb', nb),('clf', clf),('svm0', svm0) ,('svm2', svm2),('svm3',svm3),('svm4', svm4 )], voting='hard')\n",
        "ensemble3 = VotingClassifier(estimators=[('dt', dt ), ('lr', lr), ('svm', svm0),('nb',nb),('knn', knn ), ('lda', lda), ('qda', qda), ('svm2', svm2),('svm3',svm3),('svm4', svm4 ), ('svm5', svm5), ('svm6', svm6)], voting='hard')\n",
        "\n",
        "# Train the classifiers on the training set\n",
        "svm0.fit(X_train31, y_train31)\n",
        "svm3.fit(X_train31, y_train31)\n",
        "svm4.fit(X_train31, y_train31)\n",
        "svm5.fit(X_train31, y_train31)\n",
        "svm6.fit(X_train31, y_train31)\n",
        "\n",
        "dt.fit(X_train31, y_train31)\n",
        "nb.fit(X_train31, y_train31)\n",
        "knn.fit(X_train31, y_train31)\n",
        "lr.fit(X_train31, y_train31)\n",
        "#adab.fit(X_train32, y_train32)\n",
        "lda.fit(X_train31, y_train31)\n",
        "qda.fit(X_train31, y_train31)\n",
        "#xgb.fit(X_train31, y_train31)\n",
        "clf.fit(X_train31, y_train31)\n",
        "ensemble.fit(X_train31, y_train31)\n",
        "ensemble2.fit(X_train31, y_train31)\n",
        "ensemble3.fit(X_train31, y_train31)\n",
        "# Predict the test set using the trained classifiers\n",
        "svm0_pred = svm0.predict(X_test31)\n",
        "svm3_pred = svm3.predict(X_test31)\n",
        "svm4_pred = svm4.predict(X_test31)\n",
        "svm5_pred = svm5.predict(X_test31)\n",
        "svm6_pred = svm6.predict(X_test31)\n",
        "\n",
        "dt_pred = dt.predict(X_test31)\n",
        "nb_pred = nb.predict(X_test31)\n",
        "knn_pred = knn.predict(X_test31)\n",
        "lr_pred = lr.predict(X_test31)\n",
        "#adab_pred = adab.predict(X_test32)\n",
        "lda_pred = lda.predict(X_test31)\n",
        "qda_pred = qda.predict(X_test31)\n",
        "#xgb_pred = xgb.predict(X_test31)\n",
        "clf_pred = clf.predict(X_test31)\n",
        "ensemble_pred = ensemble.predict(X_test31)\n",
        "ensemble_pred2 = ensemble2.predict(X_test31)\n",
        "ensemble_pred3 = ensemble3.predict(X_test31)\n",
        "# Evaluate the accuracy of each classifier on the test set\n",
        "svm0_acc = accuracy_score(y_test31, svm0_pred)\n",
        "svm3_acc = accuracy_score(y_test31, svm3_pred)\n",
        "svm4_acc = accuracy_score(y_test31, svm4_pred)\n",
        "svm5_acc = accuracy_score(y_test31, svm5_pred)\n",
        "svm6_acc = accuracy_score(y_test31, svm6_pred)\n",
        "\n",
        "dt_acc = accuracy_score(y_test31, dt_pred)\n",
        "nb_acc = accuracy_score(y_test31, nb_pred)\n",
        "knn_acc = accuracy_score(y_test31, knn_pred)\n",
        "lr_acc = accuracy_score(y_test31, lr_pred)\n",
        "#adab_acc = accuracy_score(y_test32, adab_pred)\n",
        "lda_acc = accuracy_score(y_test31, lda_pred)\n",
        "qda_acc = accuracy_score(y_test31, qda_pred)\n",
        "#xgb_acc = accuracy_score(y_test31, xgb_pred)\n",
        "clf_acc = accuracy_score(y_test31, clf_pred)\n",
        "ensemble_acc = accuracy_score(y_test31, ensemble_pred)\n",
        "ensemble_acc2 = accuracy_score(y_test31, ensemble_pred2)\n",
        "ensemble_acc3 = accuracy_score(y_test31, ensemble_pred3)\n",
        "\n",
        "print('svm0 Classifier accuracy:', svm0_acc)\n",
        "print('svm3 Classifier accuracy:', svm3_acc)\n",
        "print('svm4 Classifier accuracy:', svm4_acc)\n",
        "print('svm5 Classifier accuracy:', svm5_acc)\n",
        "print('svm6 Classifier accuracy:', svm6_acc)\n",
        "\n",
        "print('dt Classifier accuracy:', dt_acc)\n",
        "print('nb Classifier accuracy:', nb_acc)\n",
        "print('K-Nearest Neighbors Classifier accuracy:', knn_acc)\n",
        "print('Logistic Regression Classifier accuracy:', lr_acc)\n",
        "#print('adab Classifier accuracy:', adab_acc)\n",
        "print('lda Classifier accuracy:', lda_acc)\n",
        "print('qda Classifier accuracy:', qda_acc)\n",
        "#print('xgb Classifier accuracy:', xgb_acc)\n",
        "print('clf Classifier accuracy:', clf_acc)\n",
        "print('ensemble Classifier accuracy:', ensemble_acc)\n",
        "print('ensemble2 Classifier accuracy:', ensemble_acc2)\n",
        "print('ensemble3 Classifier accuracy:', ensemble_acc3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWOLbAeiqJX8",
        "outputId": "2b5f807e-c357-4eca-9b50-cce0a1a11b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bg Classifier accuracy: 0.9977470228516253\n"
          ]
        }
      ],
      "source": [
        "# max_samples: maximum size 0.5=50% of each sample taken from the full dataset\n",
        "# max_features: maximum of features 1=100% taken here all 10K \n",
        "# n_estimators: number of decision trees \n",
        "bg=BaggingClassifier(DecisionTreeClassifier(),max_samples=0.5,max_features=1.0,n_estimators=10)\n",
        "bg.fit(X_train31, y_train31)\n",
        "bg_pred = bg.predict(X_test31)\n",
        "bg_acc = accuracy_score(y_test31, bg_pred)\n",
        "print('bg Classifier accuracy:', bg_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNJEKUDhqQ3i",
        "outputId": "ed2ffe21-418e-4d54-8e29-904a805177e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "adb Classifier accuracy: 0.9993562922433216\n"
          ]
        }
      ],
      "source": [
        "adb = AdaBoostClassifier(DecisionTreeClassifier(min_samples_split=10,max_depth=4),n_estimators=10,learning_rate=0.6)\n",
        "adb.fit(X_train31, y_train31)\n",
        "adb_pred = adb.predict(X_test31)\n",
        "adb_acc = accuracy_score(y_test31, adb_pred)\n",
        "print('adb Classifier accuracy:', adb_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UxvNGwgqmqz",
        "outputId": "c5c2494d-fe7c-4719-aa2a-022535c0ae97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rf Classifier accuracy: 0.9996781461216607\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier(max_depth=10, n_jobs=-1, random_state=0)\n",
        "rf.fit(X_train31, y_train31)\n",
        "rf_pred = rf.predict(X_test31)\n",
        "rf_acc = accuracy_score(y_test31, rf_pred)\n",
        "print('rf Classifier accuracy:', rf_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_LPFtuPyI-o",
        "outputId": "11ba87db-a348-4f54-9f36-bcc1b2d4eb93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Create a Gaussian Process Classifier with RBF kernel\n",
        "kernel = 1.0 * RBF(length_scale=1.0)\n",
        "clf = GaussianProcessClassifier(kernel=kernel, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train31, y_train31)\n",
        "\n",
        "# Predict the class labels for the test data\n",
        "y_pred = clf.predict(X_test31)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test31, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbpo50zAN4AQ"
      },
      "outputs": [],
      "source": [
        "# create a Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "\n",
        "# train the model on the training set\n",
        "gb_classifier.fit(X_train31, y_train31)\n",
        "\n",
        "# make predictions on the testing set\n",
        "y_pred = gb_classifier.predict(X_test31)\n",
        "\n",
        "# evaluate the model\n",
        "accuracy = accuracy_score(y_test31, y_pred)\n",
        "report = classification_report(y_test31, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fca4zKN_yOwc",
        "outputId": "44b8b08e-6884-4850-e175-45800f171359"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9996781461216607\n"
          ]
        }
      ],
      "source": [
        "# Create an SGD Classifier with logistic loss function\n",
        "clf = SGDClassifier(loss='log', penalty='l2', max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train31, y_train31)\n",
        "\n",
        "# Predict the class labels for the test data\n",
        "y_pred = clf.predict(X_test31)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test31, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiixeaNXzaYF",
        "outputId": "8ff794c7-7ec9-48e0-e2a4-649fd071ed23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   1.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   1.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   1.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   3.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   2.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   3.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   1.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   1.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   2.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   1.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   1.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   1.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   1.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   1.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   1.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   1.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   1.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   2.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   2.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   1.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   1.1s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   1.1s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.8s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   1.8s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   1.0s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   1.2s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   1.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   1.4s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   1.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   1.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   1.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   1.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   1.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   1.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   1.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   1.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   1.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   1.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   2.5s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   1.2s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   1.0s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   1.3s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   1.3s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   1.5s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   1.6s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   1.6s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   1.3s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   1.5s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   1.3s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   3.0s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   1.1s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   1.5s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   1.4s\n",
            "[CV] END alpha=0.001, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   2.0s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=constant, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=l2; total time=   1.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   1.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=l2; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=l2; total time=   1.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, learning_rate=optimal, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   1.1s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=l2; total time=   1.1s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=l2; total time=   1.1s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   1.2s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   1.2s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   1.1s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=l2; total time=   1.1s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=optimal, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=log, max_iter=3000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=1000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=2000, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, learning_rate=invscaling, loss=hinge, max_iter=3000, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "540 fits failed out of a total of 810.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "540 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 892, in fit\n",
            "    self._more_validate_params()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 149, in _more_validate_params\n",
            "    raise ValueError(\"eta0 must be > 0\")\n",
            "ValueError: eta0 must be > 0\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.99914163 0.99935622 0.99935622 0.99914163 0.99935622 0.99935622\n",
            " 0.99914163 0.99935622 0.99935622 0.99871245 0.99935622 0.99935622\n",
            " 0.99871245 0.99935622 0.99935622 0.99871245 0.99935622 0.99935622\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.99806867 0.99849785 0.99914163 0.99806867 0.99849785 0.99914163\n",
            " 0.99806867 0.99849785 0.99914163 0.99763948 0.99678112 0.99957082\n",
            " 0.99763948 0.99678112 0.99957082 0.99763948 0.99678112 0.99957082\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.98690987 0.99763948 0.99678112 0.98690987 0.99763948 0.99678112\n",
            " 0.98690987 0.99763948 0.99678112 0.99227468 0.99849785 0.99871245\n",
            " 0.99227468 0.99849785 0.99871245 0.99227468 0.99849785 0.99871245\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'alpha': 0.001, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'elasticnet'}\n",
            "Accuracy: 0.9993562922433216\n"
          ]
        }
      ],
      "source": [
        "# Create an SGD Classifier\n",
        "clf = SGDClassifier(random_state=42)\n",
        "\n",
        "# Define a grid of hyperparameters to search over\n",
        "param_grid = {'loss': ['log', 'hinge'], \n",
        "              'penalty': ['l1', 'l2', 'elasticnet'], \n",
        "              'alpha': [0.0001, 0.001, 0.01], \n",
        "              'max_iter': [1000, 2000, 3000], \n",
        "              'learning_rate': ['constant', 'optimal', 'invscaling']}\n",
        "\n",
        "# Use grid search to find the best set of hyperparameters\n",
        "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, verbose=2)\n",
        "grid_search.fit(X_train31, y_train31)\n",
        "\n",
        "# Print the best set of hyperparameters and the corresponding accuracy score\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "y_pred = grid_search.predict(X_test31)\n",
        "accuracy = accuracy_score(y_test31, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwtglm3002bt",
        "outputId": "8c66fa6c-e246-4b45-a2ae-17c016aa7cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Create a Multinomial Logistic Regression classifier\n",
        "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
        "\n",
        "# Train the classifier on the training set\n",
        "lr.fit(X_train31, y_train31)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = lr.predict(X_test31)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test31, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZBAdS1RQLTP"
      },
      "source": [
        "**Step 6: Obtain the result on test data using the selected model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSK1zP8n0Ujd"
      },
      "outputs": [],
      "source": [
        "#Only present the final model result here\n",
        "clf2 =  LogisticRegression(C= 1, penalty= 'l2', solver='liblinear')\n",
        "clf2.fit(xtrain, ytrain)\n",
        "# Create our predictions\n",
        "prediction2 = clf2.predict(xtest2)\n",
        "prediction2\n",
        "savetxt('lrgridsubmit.csv', prediction2, delimiter=',',fmt='%s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "md8PbQkmEVan",
        "outputId": "c1ab5436-3db7-4736-d245-6b47e9efae9f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm3 = confusion_matrix(y_test31,clf2.predict(X_test31), labels=clf2.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm3, display_labels=clf2.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvOjtH40Q3Rf"
      },
      "source": [
        "**Multi-class Classification**  \n",
        "Classify walking(1), walking_upstairs(2), walking_downstairs(3), sitting(4), standing(5), lying(6) and static postural transition(7)  \n",
        "\n",
        "The following models are included:  \n",
        "\n",
        "(1) Support Vector Machine with kenel: linear, poly, rbf, sigmoid. \n",
        "\n",
        "(2) Decision Tree \n",
        "\n",
        "(3) Guassian Naive Bayes \n",
        "\n",
        "(4) K-Nearest Neighbors \n",
        "\n",
        "(5) Logistic Regression \n",
        "\n",
        "(6) Linear Discriminant Analysis \n",
        "\n",
        "(7) Quadratic Discriminant Analysis  \n",
        "\n",
        "(8) Random Forest  \n",
        "\n",
        "(9) Voting Classifier \n",
        "\n",
        "(10) Neutral Network \n",
        "\n",
        "(11) Bagging Classifier  \n",
        "\n",
        "(12) AdaBoost Classifier \n",
        "\n",
        "(13) Gaussian Process Classifier with RBF kernel  \n",
        "\n",
        "(14) SGD Classifier \n",
        "\n",
        "(15) Multinomial Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkxlWrVvRUDH"
      },
      "source": [
        "**Step 1: Enable Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "FECkW-oLoFM2",
        "outputId": "f4a7c599-70c6-4ae6-f923-052e08f3831d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               V3            V4            V5            V6            V7  \\\n",
              "1     0.043579674  -0.005970221  -0.035054344   -0.99538116  -0.988365863   \n",
              "2     0.039480037  -0.002131276  -0.029067362  -0.998347999  -0.982944945   \n",
              "3     0.039977781  -0.005152716  -0.022650708  -0.995482127  -0.977313843   \n",
              "4     0.039784558  -0.011808778  -0.028915779    -0.9961941  -0.988568594   \n",
              "5     0.038758138  -0.002288533  -0.023862885  -0.998241335  -0.986774135   \n",
              "...           ...           ...           ...           ...           ...   \n",
              "7763  0.048048374  -0.042445161  -0.065884336  -0.195447967  -0.278326363   \n",
              "7764  0.037638604   0.006430371  -0.044344723  -0.235372031   -0.30268012   \n",
              "7765  0.037450938  -0.002724424   0.021009408    -0.2182808   -0.37808216   \n",
              "7766  0.044011045  -0.004535781  -0.051242204  -0.219202106   -0.38334992   \n",
              "7767  0.068953765   0.001810322  -0.080323431  -0.269335686  -0.366553435   \n",
              "\n",
              "                V8            V9           V10           V11           V12  \\\n",
              "1     -0.937382005  -0.995007045  -0.988815577  -0.953325201  -0.794796369   \n",
              "2     -0.971272882  -0.998701962  -0.983314831  -0.973999847  -0.802536649   \n",
              "3     -0.984759524  -0.996414838  -0.975834798  -0.985973003  -0.798477173   \n",
              "4     -0.993255602  -0.996994335  -0.988526428  -0.993135403  -0.798477173   \n",
              "5     -0.993115488  -0.998215924   -0.98647918  -0.993824553  -0.801981534   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "7763  -0.219953526  -0.282233132  -0.305861471  -0.357803275   0.267874057   \n",
              "7764  -0.232843488  -0.322482719  -0.354463664  -0.345592459   0.181270735   \n",
              "7765  -0.076950379  -0.304446475  -0.400661352   -0.19307138   0.113140826   \n",
              "7766   -0.08103469  -0.310418503  -0.380233454  -0.201006871   0.166670564   \n",
              "7767  -0.147294234  -0.377331537  -0.360597355  -0.255505293   0.321880782   \n",
              "\n",
              "      ...          V554          V555          V556          V557  \\\n",
              "1     ...  -0.012235894  -0.314848353  -0.713307812  -0.112754341   \n",
              "2     ...   0.202803809  -0.603199224  -0.860676901   0.053476955   \n",
              "3     ...   0.440079356   -0.40442749  -0.761847228  -0.118559255   \n",
              "4     ...    0.43089077   -0.13837282  -0.491604347  -0.036787973   \n",
              "5     ...   0.137734624  -0.366213617  -0.702489762   0.123320048   \n",
              "...   ...           ...           ...           ...           ...   \n",
              "7763  ...  -0.008380747  -0.596760198  -0.879025634  -0.190436861   \n",
              "7764  ...   0.209451833  -0.404417928  -0.684496226   0.064906712   \n",
              "7765  ...   0.237002763   0.000206574  -0.317314291   0.052805928   \n",
              "7766  ...   0.069365726   0.037918739  -0.356579108  -0.101360118   \n",
              "7767  ...   0.002496349   -0.40083141  -0.742971769  -0.280088053   \n",
              "\n",
              "              V558          V559          V560          V561         V562  \\\n",
              "1      0.030400372  -0.464761386  -0.018445884  -0.841558511  0.179912811   \n",
              "2     -0.007434566   -0.73262621   0.703510588  -0.845092399   0.18026111   \n",
              "3      0.177899475   0.100699208   0.808529075  -0.849230131  0.180609558   \n",
              "4     -0.012892494   0.640011043  -0.485366445  -0.848946592  0.181907092   \n",
              "5      0.122541962   0.693578288  -0.615970609     -0.848164  0.185123692   \n",
              "...            ...           ...           ...           ...          ...   \n",
              "7763   0.829718416   0.206972154  -0.425618579  -0.792291739  0.238580336   \n",
              "7764   0.875679049  -0.879032789    0.40021936  -0.772287652  0.252652789   \n",
              "7765  -0.266724365   0.864404011   0.701168816  -0.779566335  0.249121453   \n",
              "7766   0.700739689   0.936673944   -0.58947895   -0.78560327  0.246408672   \n",
              "7767  -0.007739278  -0.056087594   -0.61695645  -0.783692535  0.246784989   \n",
              "\n",
              "              V563  \n",
              "1     -0.051718416  \n",
              "2     -0.047436337  \n",
              "3     -0.042271363  \n",
              "4     -0.040826223  \n",
              "5     -0.037079903  \n",
              "...            ...  \n",
              "7763   0.056019937  \n",
              "7764   0.056251833  \n",
              "7765   0.047070771  \n",
              "7766   0.031700029  \n",
              "7767   0.042981289  \n",
              "\n",
              "[7767 rows x 561 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fdbe1230-78d3-4e8d-a2e8-c8b1223a01a5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>...</th>\n",
              "      <th>V554</th>\n",
              "      <th>V555</th>\n",
              "      <th>V556</th>\n",
              "      <th>V557</th>\n",
              "      <th>V558</th>\n",
              "      <th>V559</th>\n",
              "      <th>V560</th>\n",
              "      <th>V561</th>\n",
              "      <th>V562</th>\n",
              "      <th>V563</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.043579674</td>\n",
              "      <td>-0.005970221</td>\n",
              "      <td>-0.035054344</td>\n",
              "      <td>-0.99538116</td>\n",
              "      <td>-0.988365863</td>\n",
              "      <td>-0.937382005</td>\n",
              "      <td>-0.995007045</td>\n",
              "      <td>-0.988815577</td>\n",
              "      <td>-0.953325201</td>\n",
              "      <td>-0.794796369</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.012235894</td>\n",
              "      <td>-0.314848353</td>\n",
              "      <td>-0.713307812</td>\n",
              "      <td>-0.112754341</td>\n",
              "      <td>0.030400372</td>\n",
              "      <td>-0.464761386</td>\n",
              "      <td>-0.018445884</td>\n",
              "      <td>-0.841558511</td>\n",
              "      <td>0.179912811</td>\n",
              "      <td>-0.051718416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.039480037</td>\n",
              "      <td>-0.002131276</td>\n",
              "      <td>-0.029067362</td>\n",
              "      <td>-0.998347999</td>\n",
              "      <td>-0.982944945</td>\n",
              "      <td>-0.971272882</td>\n",
              "      <td>-0.998701962</td>\n",
              "      <td>-0.983314831</td>\n",
              "      <td>-0.973999847</td>\n",
              "      <td>-0.802536649</td>\n",
              "      <td>...</td>\n",
              "      <td>0.202803809</td>\n",
              "      <td>-0.603199224</td>\n",
              "      <td>-0.860676901</td>\n",
              "      <td>0.053476955</td>\n",
              "      <td>-0.007434566</td>\n",
              "      <td>-0.73262621</td>\n",
              "      <td>0.703510588</td>\n",
              "      <td>-0.845092399</td>\n",
              "      <td>0.18026111</td>\n",
              "      <td>-0.047436337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.039977781</td>\n",
              "      <td>-0.005152716</td>\n",
              "      <td>-0.022650708</td>\n",
              "      <td>-0.995482127</td>\n",
              "      <td>-0.977313843</td>\n",
              "      <td>-0.984759524</td>\n",
              "      <td>-0.996414838</td>\n",
              "      <td>-0.975834798</td>\n",
              "      <td>-0.985973003</td>\n",
              "      <td>-0.798477173</td>\n",
              "      <td>...</td>\n",
              "      <td>0.440079356</td>\n",
              "      <td>-0.40442749</td>\n",
              "      <td>-0.761847228</td>\n",
              "      <td>-0.118559255</td>\n",
              "      <td>0.177899475</td>\n",
              "      <td>0.100699208</td>\n",
              "      <td>0.808529075</td>\n",
              "      <td>-0.849230131</td>\n",
              "      <td>0.180609558</td>\n",
              "      <td>-0.042271363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.039784558</td>\n",
              "      <td>-0.011808778</td>\n",
              "      <td>-0.028915779</td>\n",
              "      <td>-0.9961941</td>\n",
              "      <td>-0.988568594</td>\n",
              "      <td>-0.993255602</td>\n",
              "      <td>-0.996994335</td>\n",
              "      <td>-0.988526428</td>\n",
              "      <td>-0.993135403</td>\n",
              "      <td>-0.798477173</td>\n",
              "      <td>...</td>\n",
              "      <td>0.43089077</td>\n",
              "      <td>-0.13837282</td>\n",
              "      <td>-0.491604347</td>\n",
              "      <td>-0.036787973</td>\n",
              "      <td>-0.012892494</td>\n",
              "      <td>0.640011043</td>\n",
              "      <td>-0.485366445</td>\n",
              "      <td>-0.848946592</td>\n",
              "      <td>0.181907092</td>\n",
              "      <td>-0.040826223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.038758138</td>\n",
              "      <td>-0.002288533</td>\n",
              "      <td>-0.023862885</td>\n",
              "      <td>-0.998241335</td>\n",
              "      <td>-0.986774135</td>\n",
              "      <td>-0.993115488</td>\n",
              "      <td>-0.998215924</td>\n",
              "      <td>-0.98647918</td>\n",
              "      <td>-0.993824553</td>\n",
              "      <td>-0.801981534</td>\n",
              "      <td>...</td>\n",
              "      <td>0.137734624</td>\n",
              "      <td>-0.366213617</td>\n",
              "      <td>-0.702489762</td>\n",
              "      <td>0.123320048</td>\n",
              "      <td>0.122541962</td>\n",
              "      <td>0.693578288</td>\n",
              "      <td>-0.615970609</td>\n",
              "      <td>-0.848164</td>\n",
              "      <td>0.185123692</td>\n",
              "      <td>-0.037079903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7763</th>\n",
              "      <td>0.048048374</td>\n",
              "      <td>-0.042445161</td>\n",
              "      <td>-0.065884336</td>\n",
              "      <td>-0.195447967</td>\n",
              "      <td>-0.278326363</td>\n",
              "      <td>-0.219953526</td>\n",
              "      <td>-0.282233132</td>\n",
              "      <td>-0.305861471</td>\n",
              "      <td>-0.357803275</td>\n",
              "      <td>0.267874057</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008380747</td>\n",
              "      <td>-0.596760198</td>\n",
              "      <td>-0.879025634</td>\n",
              "      <td>-0.190436861</td>\n",
              "      <td>0.829718416</td>\n",
              "      <td>0.206972154</td>\n",
              "      <td>-0.425618579</td>\n",
              "      <td>-0.792291739</td>\n",
              "      <td>0.238580336</td>\n",
              "      <td>0.056019937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7764</th>\n",
              "      <td>0.037638604</td>\n",
              "      <td>0.006430371</td>\n",
              "      <td>-0.044344723</td>\n",
              "      <td>-0.235372031</td>\n",
              "      <td>-0.30268012</td>\n",
              "      <td>-0.232843488</td>\n",
              "      <td>-0.322482719</td>\n",
              "      <td>-0.354463664</td>\n",
              "      <td>-0.345592459</td>\n",
              "      <td>0.181270735</td>\n",
              "      <td>...</td>\n",
              "      <td>0.209451833</td>\n",
              "      <td>-0.404417928</td>\n",
              "      <td>-0.684496226</td>\n",
              "      <td>0.064906712</td>\n",
              "      <td>0.875679049</td>\n",
              "      <td>-0.879032789</td>\n",
              "      <td>0.40021936</td>\n",
              "      <td>-0.772287652</td>\n",
              "      <td>0.252652789</td>\n",
              "      <td>0.056251833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7765</th>\n",
              "      <td>0.037450938</td>\n",
              "      <td>-0.002724424</td>\n",
              "      <td>0.021009408</td>\n",
              "      <td>-0.2182808</td>\n",
              "      <td>-0.37808216</td>\n",
              "      <td>-0.076950379</td>\n",
              "      <td>-0.304446475</td>\n",
              "      <td>-0.400661352</td>\n",
              "      <td>-0.19307138</td>\n",
              "      <td>0.113140826</td>\n",
              "      <td>...</td>\n",
              "      <td>0.237002763</td>\n",
              "      <td>0.000206574</td>\n",
              "      <td>-0.317314291</td>\n",
              "      <td>0.052805928</td>\n",
              "      <td>-0.266724365</td>\n",
              "      <td>0.864404011</td>\n",
              "      <td>0.701168816</td>\n",
              "      <td>-0.779566335</td>\n",
              "      <td>0.249121453</td>\n",
              "      <td>0.047070771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7766</th>\n",
              "      <td>0.044011045</td>\n",
              "      <td>-0.004535781</td>\n",
              "      <td>-0.051242204</td>\n",
              "      <td>-0.219202106</td>\n",
              "      <td>-0.38334992</td>\n",
              "      <td>-0.08103469</td>\n",
              "      <td>-0.310418503</td>\n",
              "      <td>-0.380233454</td>\n",
              "      <td>-0.201006871</td>\n",
              "      <td>0.166670564</td>\n",
              "      <td>...</td>\n",
              "      <td>0.069365726</td>\n",
              "      <td>0.037918739</td>\n",
              "      <td>-0.356579108</td>\n",
              "      <td>-0.101360118</td>\n",
              "      <td>0.700739689</td>\n",
              "      <td>0.936673944</td>\n",
              "      <td>-0.58947895</td>\n",
              "      <td>-0.78560327</td>\n",
              "      <td>0.246408672</td>\n",
              "      <td>0.031700029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7767</th>\n",
              "      <td>0.068953765</td>\n",
              "      <td>0.001810322</td>\n",
              "      <td>-0.080323431</td>\n",
              "      <td>-0.269335686</td>\n",
              "      <td>-0.366553435</td>\n",
              "      <td>-0.147294234</td>\n",
              "      <td>-0.377331537</td>\n",
              "      <td>-0.360597355</td>\n",
              "      <td>-0.255505293</td>\n",
              "      <td>0.321880782</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002496349</td>\n",
              "      <td>-0.40083141</td>\n",
              "      <td>-0.742971769</td>\n",
              "      <td>-0.280088053</td>\n",
              "      <td>-0.007739278</td>\n",
              "      <td>-0.056087594</td>\n",
              "      <td>-0.61695645</td>\n",
              "      <td>-0.783692535</td>\n",
              "      <td>0.246784989</td>\n",
              "      <td>0.042981289</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7767 rows × 561 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fdbe1230-78d3-4e8d-a2e8-c8b1223a01a5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fdbe1230-78d3-4e8d-a2e8-c8b1223a01a5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fdbe1230-78d3-4e8d-a2e8-c8b1223a01a5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "dataset2=dataset\n",
        "List3=['7','8','9','10','11','12']\n",
        "dataset2[\"V0\"] = np.where(dataset2['V2'].isin(List3),'7',dataset2['V2'])\n",
        "\n",
        "ytrain2=dataset2[\"V0\"]\n",
        "ytrain2=ytrain2[1:]\n",
        "xtrain2=dataset2.iloc[1:,3:564]\n",
        "xtrain2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN6_6tCTRddk"
      },
      "source": [
        "**Step 2: Split data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0AaGTeyv1qjv"
      },
      "outputs": [],
      "source": [
        "#I tried several ways to split the data, including 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4. \n",
        "#Considering the limited space here, this version of code only including the 0.4, \n",
        "#which meeans divide the data into train(60%) and test(40%) data. \n",
        "#You can easily change the fraction as you want.\n",
        "X_train32, X_test32, y_train32, y_test32 = train_test_split(xtrain2, ytrain2 , test_size=0.4, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVo929_jRvln"
      },
      "source": [
        "**Step 3: Build base model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "sbJMBNHYoSxB",
        "outputId": "dd06aa1b-6ef6-4423-860a-a03a5eddee3d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initialize Estimator\n",
        "dummy_clf2 = DummyClassifier(strategy='stratified')\n",
        "dummy_clf2.fit(X_train32,y_train32)\n",
        "\n",
        "# Check for Model Accuracy\n",
        "\n",
        "cm2 = confusion_matrix(y_test32,dummy_clf2.predict(X_test32), labels=dummy_clf2.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=dummy_clf2.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umLtjM3mR1SX"
      },
      "source": [
        "**Step 4: Obtain optimal parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C_E0jR5GpPog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38cbd963-8f95-4dfc-ecb9-cb7dfe598a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
          ]
        }
      ],
      "source": [
        "# create decision tree classifier\n",
        "dtc = DecisionTreeClassifier()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 4, 6, 8, 10],\n",
        "    'min_samples_split': [2, 4, 6, 8],\n",
        "    'min_samples_leaf': [1, 2, 3, 4]\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(dtc, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLFqnW5opXS6"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid for random_state\n",
        "param_grid = {'random_state': range(0, 101)}\n",
        "\n",
        "# Define the SVM classifier\n",
        "svc = SVC()\n",
        "\n",
        "# Define the grid search object\n",
        "grid_search = GridSearchCV(svc, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search object to the data\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# Print the best random_state parameter value and the corresponding mean test score\n",
        "print('Best random_state:', grid_search.best_params_['random_state'])\n",
        "print('Mean test score:', grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01qg62hCqBRp"
      },
      "outputs": [],
      "source": [
        "# create KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrX_rkwkppE3"
      },
      "outputs": [],
      "source": [
        "# create LDA classifier\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'solver': ['svd', 'lsqr'],\n",
        "    'shrinkage': [None, 'auto', 0.1, 0.5, 0.9],\n",
        "    'n_components': [1, 2, 3]\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(lda, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ-px60lpvhM"
      },
      "outputs": [],
      "source": [
        "# create QDA classifier\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'reg_param': [0.0, 0.1, 0.5, 0.9]\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(qda, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CmEP07ypbGr"
      },
      "outputs": [],
      "source": [
        "# create logistic regression classifier\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(lr, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDpz5GHXqzam"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 20],\n",
        "    'class_weight': [None, 'balanced'],\n",
        "    'n_jobs': [-1, 1, 2],\n",
        "    'random_state': [0,42, 123, 456]\n",
        "}\n",
        "\n",
        "# Create the random forest classifier\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Perform the grid search using cross-validation\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# Print the best hyperparameters and the corresponding validation score\n",
        "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Validation score: {grid_search.best_score_:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oYtvZwhq_pS"
      },
      "outputs": [],
      "source": [
        "# define parameter grid\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100], 'kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
        "\n",
        "# create SVM classifier\n",
        "svc = SVC()\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# print optimal parameters\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1RNM85dSDQt"
      },
      "source": [
        "**Step 5: Training data evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0VbSeRpfoeTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4823e1b-02ad-48bb-9050-7f3bdb001c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "svm0 Classifier accuracy: 0.9851947215963952\n",
            "svm3 Classifier accuracy: 0.9868039909880915\n",
            "svm4 Classifier accuracy: 0.9880914065014483\n",
            "svm5 Classifier accuracy: 0.9703894431927905\n",
            "svm6 Classifier accuracy: 0.7985194721596395\n",
            "dt Classifier accuracy: 0.8361763759253299\n",
            "nb Classifier accuracy: 0.6369488252333441\n",
            "K-Nearest Neighbors Classifier accuracy: 0.9510782104924365\n",
            "Logistic Regression Classifier accuracy: 0.9839073060830383\n",
            "lda Classifier accuracy: 0.9803669134213068\n",
            "qda Classifier accuracy: 0.9114901834567106\n",
            "clf Classifier accuracy: 0.9732861280978435\n",
            "ensemble Classifier accuracy: 0.989056968136466\n"
          ]
        }
      ],
      "source": [
        "# Build multiple classifiers\n",
        "svm0 = LinearSVC()\n",
        "dt = DecisionTreeClassifier( max_depth=4, min_samples_leaf= 2,min_samples_split= 4)\n",
        "nb = GaussianNB()\n",
        "knn = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\n",
        "lr = LogisticRegression(random_state=22)\n",
        "# adab = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100)\n",
        "lda = LinearDiscriminantAnalysis(n_components=1, shrinkage=None, solver='svd')\n",
        "qda = QuadraticDiscriminantAnalysis(reg_param= 0.1)\n",
        "#xgb = xgb.XGBClassifier()\n",
        "svm2 = SVC(random_state=0)\n",
        "clf = RandomForestClassifier(max_depth=10, n_jobs=-1, random_state=0)\n",
        "svm3 = SVC(C=0.1,gamma=0.1,kernel='linear')\n",
        "svm4 = SVC(kernel='poly')\n",
        "svm5 = SVC(kernel='rbf')\n",
        "svm6 = SVC(kernel='sigmoid')\n",
        "# Define the ensemble classifier\n",
        "ensemble = VotingClassifier(estimators=[('dt', svm0), ('lr', svm3), ('svm', svm4)], voting='hard')\n",
        "\n",
        "\n",
        "# Train the classifiers on the training set\n",
        "svm0.fit(X_train32, y_train32)\n",
        "svm3.fit(X_train32, y_train32)\n",
        "svm4.fit(X_train32, y_train32)\n",
        "svm5.fit(X_train32, y_train32)\n",
        "svm6.fit(X_train32, y_train32)\n",
        "dt.fit(X_train32, y_train32)\n",
        "nb.fit(X_train32, y_train32)\n",
        "knn.fit(X_train32, y_train32)\n",
        "lr.fit(X_train32, y_train32)\n",
        "#adab.fit(X_train32, y_train32)\n",
        "lda.fit(X_train32, y_train32)\n",
        "qda.fit(X_train32, y_train32)\n",
        "#xgb.fit(X_train31, y_train31)\n",
        "clf.fit(X_train32, y_train32)\n",
        "ensemble.fit(X_train32, y_train32)\n",
        "\n",
        "# Predict the test set using the trained classifiers\n",
        "svm0_pred = svm0.predict(X_test32)\n",
        "svm3_pred = svm3.predict(X_test32)\n",
        "svm4_pred = svm4.predict(X_test32)\n",
        "svm5_pred = svm5.predict(X_test32)\n",
        "svm6_pred = svm6.predict(X_test32)\n",
        "\n",
        "dt_pred = dt.predict(X_test32)\n",
        "nb_pred = nb.predict(X_test32)\n",
        "knn_pred = knn.predict(X_test32)\n",
        "lr_pred = lr.predict(X_test32)\n",
        "#adab_pred = adab.predict(X_test32)\n",
        "lda_pred = lda.predict(X_test32)\n",
        "qda_pred = qda.predict(X_test32)\n",
        "#xgb_pred = xgb.predict(X_test31)\n",
        "clf_pred = clf.predict(X_test32)\n",
        "ensemble_pred = ensemble.predict(X_test32)\n",
        "\n",
        "# Evaluate the accuracy of each classifier on the test set\n",
        "svm0_acc = accuracy_score(y_test32, svm0_pred)\n",
        "svm3_acc = accuracy_score(y_test32, svm3_pred)\n",
        "svm4_acc = accuracy_score(y_test32, svm4_pred)\n",
        "svm5_acc = accuracy_score(y_test32, svm5_pred)\n",
        "svm6_acc = accuracy_score(y_test32, svm6_pred)\n",
        "\n",
        "dt_acc = accuracy_score(y_test32, dt_pred)\n",
        "nb_acc = accuracy_score(y_test32, nb_pred)\n",
        "knn_acc = accuracy_score(y_test32, knn_pred)\n",
        "lr_acc = accuracy_score(y_test32, lr_pred)\n",
        "#adab_acc = accuracy_score(y_test32, adab_pred)\n",
        "lda_acc = accuracy_score(y_test32, lda_pred)\n",
        "qda_acc = accuracy_score(y_test32, qda_pred)\n",
        "#xgb_acc = accuracy_score(y_test31, xgb_pred)\n",
        "clf_acc = accuracy_score(y_test32, clf_pred)\n",
        "ensemble_acc = accuracy_score(y_test32, ensemble_pred)\n",
        "\n",
        "print('svm0 Classifier accuracy:', svm0_acc)\n",
        "print('svm3 Classifier accuracy:', svm3_acc)\n",
        "print('svm4 Classifier accuracy:', svm4_acc)\n",
        "print('svm5 Classifier accuracy:', svm5_acc)\n",
        "print('svm6 Classifier accuracy:', svm6_acc)\n",
        "\n",
        "print('dt Classifier accuracy:', dt_acc)\n",
        "print('nb Classifier accuracy:', nb_acc)\n",
        "print('K-Nearest Neighbors Classifier accuracy:', knn_acc)\n",
        "print('Logistic Regression Classifier accuracy:', lr_acc)\n",
        "#print('adab Classifier accuracy:', adab_acc)\n",
        "print('lda Classifier accuracy:', lda_acc)\n",
        "print('qda Classifier accuracy:', qda_acc)\n",
        "#print('xgb Classifier accuracy:', xgb_acc)\n",
        "print('clf Classifier accuracy:', clf_acc)\n",
        "print('ensemble Classifier accuracy:', ensemble_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jCzvz7e4ovli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dfbdd56-40d2-40bc-a797-8453b7159750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bg Classifier accuracy: 0.9559060186675249\n"
          ]
        }
      ],
      "source": [
        "# max_samples: maximum size 0.5=50% of each sample taken from the full dataset\n",
        "# max_features: maximum of features 1=100% taken here all 10K \n",
        "# n_estimators: number of decision trees \n",
        "bg=BaggingClassifier(DecisionTreeClassifier(),max_samples=0.5,max_features=1.0,n_estimators=10)\n",
        "bg.fit(X_train32, y_train32)\n",
        "bg_pred = bg.predict(X_test32)\n",
        "bg_acc = accuracy_score(y_test32, bg_pred)\n",
        "print('bg Classifier accuracy:', bg_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UN2Oj18GqWCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed74233c-9a97-45ad-ff26-2e13149240f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adb Classifier accuracy: 0.918892822658513\n"
          ]
        }
      ],
      "source": [
        "adb = AdaBoostClassifier(DecisionTreeClassifier(min_samples_split=10,max_depth=4),n_estimators=10,learning_rate=0.6)\n",
        "adb.fit(X_train32, y_train32)\n",
        "adb_pred = adb.predict(X_test32)\n",
        "adb_acc = accuracy_score(y_test32, adb_pred)\n",
        "print('adb Classifier accuracy:', adb_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kf3oIaegqfel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1d4970-5ad6-4217-c15d-df7d236d69e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rf Classifier accuracy: 0.9732861280978435\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier(max_depth=10, n_jobs=-1, random_state=0)\n",
        "rf.fit(X_train32, y_train32)\n",
        "rf_pred = rf.predict(X_test32)\n",
        "rf_acc = accuracy_score(y_test32, rf_pred)\n",
        "print('rf Classifier accuracy:', rf_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXrNWohuyCXN"
      },
      "outputs": [],
      "source": [
        "# Create a Gaussian Process Classifier with RBF kernel\n",
        "kernel = 1.0 * RBF(length_scale=1.0)\n",
        "clf = GaussianProcessClassifier(kernel=kernel, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train32, y_train32)\n",
        "\n",
        "# Predict the class labels for the test data\n",
        "y_pred = clf.predict(X_test32)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test32, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghMP8DCGyTMJ"
      },
      "outputs": [],
      "source": [
        "# Create an SGD Classifier with logistic loss function\n",
        "clf = SGDClassifier(loss='log', penalty='l2', max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train32, y_train32)\n",
        "\n",
        "# Predict the class labels for the test data\n",
        "y_pred = clf.predict(X_test32)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test32, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqLNDepFzViL"
      },
      "outputs": [],
      "source": [
        "# Create an SGD Classifier\n",
        "clf = SGDClassifier(random_state=42)\n",
        "\n",
        "# Define a grid of hyperparameters to search over\n",
        "param_grid = {'loss': ['log', 'hinge'], \n",
        "              'penalty': ['l1', 'l2', 'elasticnet'], \n",
        "              'alpha': [0.0001, 0.001, 0.01], \n",
        "              'max_iter': [1000, 2000, 3000], \n",
        "              'learning_rate': ['constant', 'optimal', 'invscaling']}\n",
        "\n",
        "# Use grid search to find the best set of hyperparameters\n",
        "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, verbose=2)\n",
        "grid_search.fit(X_train32, y_train32)\n",
        "\n",
        "# Print the best set of hyperparameters and the corresponding accuracy score\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "y_pred = grid_search.predict(X_test32)\n",
        "accuracy = accuracy_score(y_test32, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX_IwTde0sZz"
      },
      "outputs": [],
      "source": [
        "# Create a Multinomial Logistic Regression classifier\n",
        "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
        "\n",
        "# Train the classifier on the training set\n",
        "lr.fit(X_train32, y_train32)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = lr.predict(X_test32)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test32, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yKFP498W-dX"
      },
      "source": [
        "Prepare data for network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGsNb-9OwVqd"
      },
      "outputs": [],
      "source": [
        "xtrain = xtrain.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# print the converted DataFrame\n",
        "print(xtrain)\n",
        "xtrain.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKKUHw90wV0Q"
      },
      "outputs": [],
      "source": [
        "xtest2 = xtest2.apply(pd.to_numeric, errors='coerce')\n",
        "print(xtest2)\n",
        "xtest2.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAQ4s7MNwc-J"
      },
      "outputs": [],
      "source": [
        "ytrain = ytrain.apply(pd.to_numeric, errors='coerce')\n",
        "print(ytrain)\n",
        "ytrain.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOf19_2u1hH8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train32, X_test32, y_train32, y_test32 = train_test_split(xtrain, ytrain , test_size=0.4, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34u3w9XXwdvP"
      },
      "outputs": [],
      "source": [
        "X_train32 = X_train32.apply(pd.to_numeric)\n",
        "\n",
        "# convert dataframe to numpy array\n",
        "my_array = np.array(X_train32)\n",
        "\n",
        "print(my_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3r6hj-Wwe4u"
      },
      "outputs": [],
      "source": [
        "X_test32 = X_test32.apply(pd.to_numeric)\n",
        "\n",
        "# convert dataframe to numpy array\n",
        "my_arrayyt = np.array(X_test32)\n",
        "\n",
        "print(my_arrayyt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDOVFpwXwe75"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# convert object dtype columns to numeric\n",
        "y_train32 = y_train32.apply(pd.to_numeric)\n",
        "\n",
        "# convert dataframe to numpy array\n",
        "my_arrayy = np.array(y_train32)\n",
        "\n",
        "print(my_arrayy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E02ggDZvwjGK"
      },
      "outputs": [],
      "source": [
        "y_test32 = y_test32.apply(pd.to_numeric)\n",
        "\n",
        "# convert dataframe to numpy array\n",
        "my_arrayytt = np.array(y_test32)\n",
        "\n",
        "print(my_arrayytt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFrHiEAmwWOo"
      },
      "outputs": [],
      "source": [
        "# create a sample dataset\n",
        "X = my_array\n",
        "y = my_arrayy\n",
        "Xt = my_arrayyt\n",
        "yt = my_arrayytt\n",
        "# define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=561, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model.fit(X, y, epochs=100, batch_size=10)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(Xt, yt)\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "# make predictions\n",
        "predictions = model.predict(Xt)\n",
        "print(\"Predictions: \", predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q50vuv-5XIjo"
      },
      "source": [
        "**Step 6: Obtain result on test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVddmE7A1LOC"
      },
      "outputs": [],
      "source": [
        "#Only show the final chosen model here\n",
        "ensemble = SVC(C=1,kernel='linear')\n",
        "ensemble.fit(xtrain2, ytrain2)\n",
        "# make a single prediction\n",
        "ensemble.score(xtrain2,ytrain2)\n",
        "yhat2 = ensemble.predict(xtest2)\n",
        "yhat2\n",
        "savetxt('linearsvmc1.csv', yhat2, delimiter=',',fmt='%s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "TnFoH6TEFIQk",
        "outputId": "92b4a917-4b1b-4692-fe90-76ef4c6c980d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAGwCAYAAACuFMx9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTZElEQVR4nO3deXgT5fo38O8kadM16UIXShf2QtlFD1RQFpGKiCi4cSoURXjFgiwHRI4om1D0KCL+WEQRXKiIC4iIrEoBWYQCylp2W6Ablu40zTLvH9hApEDTJJ1k8v1c11wXmXlmct8kcOd55pkZQRRFEURERCRbCqkDICIiIsdisSciIpI5FnsiIiKZY7EnIiKSORZ7IiIimWOxJyIikjkWeyIiIplTSR2ALUwmEy5dugR/f38IgiB1OEREZCVRFFFSUoKIiAgoFI7rf1ZUVKCystLm43h6esLLy8sOEdUtly72ly5dQlRUlNRhEBGRjbKyshAZGemQY1dUVKBRjB9y8ow2Hys8PBznzp1zuYLv0sXe398fADBve3t4+ykljsZ+vrzLMV94IiJnY4AeO7He/P+5I1RWViInz4g/0xtC41/70YPiEhNiOp5HZWUli31dqhq69/ZTwtvPpVOxoBI8pA6BiKhu/H3D9ro4FevnL8DPv/bvY4Lrni6WT4UkIiK6DaNogtGGp8EYRZP9gqljLPZEROQWTBBhQu2rvS37So2X3hEREckce/ZEROQWTDDBloF42/aWFos9ERG5BaMowijWfijeln2lxmF8IiIimWPPnoiI3II7T9BjsSciIrdgggijmxZ7DuMTERHJHHv2RETkFjiMT0REJHOcjU9ERESyxZ49ERG5BdPfiy37uyoWeyIicgtGG2fj27Kv1FjsiYjILRhF2PjUO/vFUtd4zp6IiEjm2LMnIiK3wHP2REREMmeCACMEm/Z3VSz2fzu8xB8H3w1AyyEluOe1QgDA1XwF0t8OwKVdXjCUCdA0MqDNi8WISbhq3q/4nAr73w5A/gFPmPQCAmL16DCmCOGddRJlUjP9hl7GEyPzEBRiwNlj3lg4pQEyDvlIHZZN5JaT3PIBmJOrkGNO7o7n7AFc/sMTp1b6ITC20mL9zknBKDqnQs9Fl9HvhxxEP3gV28cG469jHuY2W1+sB9EI9P40H32/y0VQi0r8/GI9XM133r/abo9ewYipl7BibjiSE5rj7DEvzEo9C22wXurQak1uOcktH4A5uQo55lTFJNq+uCpJK9L27dvRr18/REREQBAErFmzps5j0JcJ2DExCJ3fLICn1vKMTP5BT7R4thT12lbCP8qIti8Vw0NjQsFRTwBARYECJec90HpEMQJb6KFpaMBd/ymC4aoCV055VPd2TmHAiMvYkBqETV8FIfOUF+ZPioTuqoCEQQVSh1ZrcstJbvkAzMlVyDGnKsa/h/FtWVyVpMW+rKwM7dq1w4IFCySLYe+MQER2q0DEvTcPu4d0qMT5n3ygK1RANAHnfvSGSScg7F/X2qoDTdA00uPMGl/oywWYDMDJr/zgFWxEcKvKm47nDFQeJjRrW44DO/zN60RRwMEd/ojrWC5hZLUnt5zklg/AnFyFHHOiayQ9Z9+nTx/06dNHsvc/96M3Co55oO83udVu7zbvMtLGBeOrTg0gqESovER0/7/L0MQYAACCADy4PB/bXqqHL+9qAEEBeAWZ8MDH+VBrnXO8RxNkhFIFFOZbfvRXLqsQ1dS55xncitxykls+AHNyFXLM6Ua29s5duWfvUhP0dDoddLrrX7ji4uJaH6ssW4l9swLx4Cf5UKqrb3PwfS30xQo8uDwP6kATsrZ4I21sPTy0Ig+BsXqIIrB3eiC8go14aMUVKL1EnPraD7+8WA8Pf5MLn1BXvlCDiEheTKIAk2jDbHwb9pWaSxX7lJQUTJ8+3S7H+uuoJyr+UmLdgDDzOtEoIHefGidW+OGxDdnI+MIfj67LRkCzaz35oBZ65O5XI2OFHzrPuIKcPWpc3OaFp/ddhKfftZ58cKsrWL0rHGfW+KLNiBK7xGpPxQVKGA1AQIjBYn1gPQOu5LvU18FMbjnJLR+AObkKOeZE1zjvlPFqTJ48GUVFReYlKyur1seq37kC/X7IwSNrcs1LcOtKNO5XjkfW5MJw9e+/mn/8DQlKEVVPOTRcvfYrT/jHjz1BgNPefcGgV+DUHz7o0PX6DxFBENG+aymOpbvmpTVyy0lu+QDMyVXIMacbufMEPZf6qaZWq6FW32LM3UoefiICm1teSqLyMUEdYEJgcz1MesA/Ro89bwTi7klFUAcYkbnFB9m/eqHnh5cBACHtK+GpMeHXV4PQNrkYKrWIk6t8UXpRhQbdK+wSpyN8t6QeJszLwsnffZBx0AePD8+Hl48Jm1YGSR1arcktJ7nlAzAnVyHHnKoYoYDRhj6u0Y6x1DWXKvZ1SeEBPLDkMg68q8XPL9aDoVyAf7QBXeYUILLbtUJeNRnv4DwtNiWFQNQL0DbTo8eCywhq4bzXpKatDYQ22IghE3MQGGLA2aPeeC2xEQovO+/lgncit5zklg/AnFyFHHOqItp4zl504XP2giiKkk0bLy0txenTpwEAHTp0wNy5c9GjRw8EBQUhOjr6jvsXFxdDq9XiwwMd4e0nn98tn8VGSR0CEVGdMIh6bMP3KCoqgkajcch7VNWKrYej4etf+559WYkJD7TJdGisjiJphdy/fz969Ohhfj1+/HgAQFJSEpYvXy5RVEREJEe89E4i3bt3h4QDC0RE5EaMogJG0YZz9i5crlxqNj4RERFZTz4nuomIiG7DBAEmG/q4Jrhu157FnoiI3II7n7PnMD4REZHMsWdPRERuwfYJehzGJyIicmrXztnb8CAcDuMTERHRjaZNmwZBECyWFi1amLdXVFQgOTkZwcHB8PPzw8CBA5Gba/nI9czMTPTt2xc+Pj4IDQ3FxIkTYTAY/vlWd8SePRERuQWTjffGr81s/FatWmHLli3m1yrV9bI7btw4/Pjjj/j666+h1WoxatQoDBgwAL/++isAwGg0om/fvggPD8euXbuQnZ2NIUOGwMPDA7Nnz7YqDhZ7IiJyC1Kcs1epVAgPD79pfVFREZYuXYrU1FT07NkTALBs2TK0bNkSe/bsQefOnbFp0yYcO3YMW7ZsQVhYGNq3b4+ZM2di0qRJmDZtGjw9PWscB4fxiYjILZigsHkBrt1r/8ZFp9Pd8j1PnTqFiIgING7cGImJicjMzAQApKenQ6/Xo1evXua2LVq0QHR0NHbv3g0A2L17N9q0aYOwsDBzm4SEBBQXF+Po0aNW5c5iT0REZIWoqChotVrzkpKSUm27Tp06Yfny5diwYQMWLVqEc+fO4b777kNJSQlycnLg6emJgIAAi33CwsKQk5MDAMjJybEo9FXbq7ZZg8P4RETkFoyiAKMNj6mt2jcrK8viqXdqtbra9n369DH/uW3btujUqRNiYmKwatUqeHt71zqO2mDPnoiI3ILx7wl6tiwAoNFoLJZbFft/CggIQPPmzXH69GmEh4ejsrIShYWFFm1yc3PN5/jDw8Nvmp1f9bq6eQC3w2JPRERUB0pLS3HmzBnUr18fHTt2hIeHB7Zu3WrenpGRgczMTMTHxwMA4uPjcfjwYeTl5ZnbbN68GRqNBnFxcVa9N4fxiYjILZhEBUw2zMY3WTkbf8KECejXrx9iYmJw6dIlTJ06FUqlEoMGDYJWq8WwYcMwfvx4BAUFQaPRYPTo0YiPj0fnzp0BAL1790ZcXBwGDx6Mt99+Gzk5OZgyZQqSk5NrPJpQhcWeiIjcwo1D8bXb37pif+HCBQwaNAh//fUXQkJC0LVrV+zZswchISEAgPfeew8KhQIDBw6ETqdDQkICFi5caN5fqVRi3bp1GDlyJOLj4+Hr64ukpCTMmDHD6thZ7ImIiBxg5cqVt93u5eWFBQsWYMGCBbdsExMTg/Xr19scC4s9ERG5BRNg02x8k/1CqXMs9kRE5BZuvDFObfd3VbIo9l/eFQmV4CF1GHYT+GuQ1CHY3ZUuBVKHQETktmRR7ImIiO7E9nvjs2dPRETk1Nz5efYs9kRE5BbcuWfvupETERFRjbBnT0REbsH2m+q4bv+YxZ6IiNyCSRRgsuU6exv2lZrr/kwhIiKiGmHPnoiI3ILJxmF83lSHiIjIydn+1DvXLfauGzkRERHVCHv2RETkFowQYLThxji27Cs1FnsiInILHMYnIiIi2WLPnoiI3IIRtg3FG+0XSp1jsSciIrfgzsP4LPZEROQW+CAcIiIiki327ImIyC2INj7PXuSld0RERM6Nw/hEREQkW+zZ10C/oZfxxMg8BIUYcPaYNxZOaYCMQz5Sh3VHFZ9fxdXFV6F+Ug2fsb4AAN33FajcXAlDhgEoB7QbAqDwv/6bT39Aj9LRJdUez/9jDVQtnfcr46qf063ILR+AObkKOeYE8BG3dBvdHr2CEVMvYcXccCQnNMfZY16YlXoW2mC91KHdluG4AbrvdVA2VVqsFysAj04e8B7iXe1+qjYqaNcGWCye/dRQRCigbKGsdh9n4Kqf063ILR+AObkKOeZUxfj3U+9sWVyVpJGnpKTgnnvugb+/P0JDQ/HYY48hIyNDypBuMmDEZWxIDcKmr4KQecoL8ydFQndVQMKgAqlDuyWxXETZ9FL4TPKF4G/5S9TraS94DfaGslX1PXTBQ4AiWGFeBK0A/Y5KeD6shiA4769aV/ycbkdu+QDMyVXIMSeSuNinpaUhOTkZe/bswebNm6HX69G7d2+UlZVJGZaZysOEZm3LcWCHv3mdKAo4uMMfcR3LJYzs9srfLYNHvAc87vGw+Vj6HXqIxSLUfdV2iMwxXPVzuhW55QMwJ1chx5xuVDWMb8viqiQ9AbthwwaL18uXL0doaCjS09Nx//3339Rep9NBp9OZXxcXFzs0Pk2QEUoVUJhv+dd05bIKUU11t9hLWpVbdDCcNELzscYux9Ot00H1Lw8oQp13+MoVP6fbkVs+AHNyFXLM6UYmKGCyoY9ry75Sc6rIi4qKAABBQUHVbk9JSYFWqzUvUVFRdRme0zPlGlE+rxy+U30hqG3/BWrKM8Hwmx7qR5y3V09ERHfmNFOrTSYTxo4diy5duqB169bVtpk8eTLGjx9vfl1cXOzQgl9coITRAASEGCzWB9Yz4Eq+0/zVmRkyjBCviCh5/oYRDyNgOGSA7jsdAn4JhKCs+Y8A3Y86CBoBHvfZfjrAkVztc7oTueUDMCdXIcecbmQUBRhtGIq3ZV+pOU3PPjk5GUeOHMHKlStv2UatVkOj0VgsjmTQK3DqDx906Hr9UjRBENG+aymOpTvfZSgeHT2g+VwDzfLri7KFEp69PaFZrrGq0IuiiMr1Onj2UUNQOfcX3NU+pzuRWz4Ac3IVcszpRjxnL7FRo0Zh3bp12L59OyIjI6UOx8J3S+phwrwsnPzdBxkHffD48Hx4+ZiwaWX1pxqkJPgKUDa2/EgFbwGC5vp601+ma8uFaw9rNJ4xwuRjgiJcAYXm+m8/Q7oBpksmqPu5xhC+K31ONSG3fADm5CrkmFMV0can3okufAc9SYu9KIoYPXo0Vq9ejW3btqFRo0ZShlOttLWB0AYbMWRiDgJDDDh71BuvJTZC4WXnHtq+Fd2aClR8UmF+XZp87Re8z399LWbc69bpoGyjgjLGea+tv5HcPie55QMwJ1chx5wIEERRFKV685deegmpqan4/vvvERsba16v1Wrh7V39TV9uVFxcDK1Wi+7oD5Ugny9i4K+u/wv6n6504TW6RHQzg6jHNnyPoqIih52araoVw9Kegqdf7WtFZakeS7utcmisjiJpz37RokUAgO7du1usX7ZsGYYOHVr3ARERkWyZRNtueWuSrGtsO8mH8YmIiMixnGKCHhERkaOZbJygZ8u+UmOxJyIit2CCABNsGMa3YV+pue7PFCIiIqoR9uyJiMgtuPMd9FjsiYjILbjzOXvXjZyIiIhqhD17IiJyCybYdn97V56gx2JPRERuQbRxNr7IYk9EROTcbH1ynSs/9Y7n7ImIiGSOPXsiInIL7jwbn8WeiIjcAofxiYiISLbYsyciIrfgzvfGZ7EnIiK3wGF8IiIiki327ImIyC24c8+exZ6IiNyCOxd7DuMTERE52Jw5cyAIAsaOHWteV1FRgeTkZAQHB8PPzw8DBw5Ebm6uxX6ZmZno27cvfHx8EBoaiokTJ8JgMFj9/uzZO6ErXQqkDsHuvNPCpA7Brq52y71zIyJyKlL17Pft24cPP/wQbdu2tVg/btw4/Pjjj/j666+h1WoxatQoDBgwAL/++isAwGg0om/fvggPD8euXbuQnZ2NIUOGwMPDA7Nnz7YqBvbsiYjILYi4fvldbRaxFu9ZWlqKxMREfPTRRwgMDDSvLyoqwtKlSzF37lz07NkTHTt2xLJly7Br1y7s2bMHALBp0yYcO3YMX3zxBdq3b48+ffpg5syZWLBgASorK62Kg8WeiIjcQlXP3pYFAIqLiy0WnU53y/dMTk5G37590atXL4v16enp0Ov1FutbtGiB6Oho7N69GwCwe/dutGnTBmFh10dGExISUFxcjKNHj1qVO4s9ERGRFaKioqDVas1LSkpKte1WrlyJAwcOVLs9JycHnp6eCAgIsFgfFhaGnJwcc5sbC33V9qpt1uA5eyIicgv2OmeflZUFjUZjXq9Wq29qm5WVhTFjxmDz5s3w8vKq9XvaC3v2RETkFuw1jK/RaCyW6op9eno68vLycNddd0GlUkGlUiEtLQ3z58+HSqVCWFgYKisrUVhYaLFfbm4uwsPDAQDh4eE3zc6vel3VpqZY7ImIiOzsgQcewOHDh3Ho0CHzcvfddyMxMdH8Zw8PD2zdutW8T0ZGBjIzMxEfHw8AiI+Px+HDh5GXl2dus3nzZmg0GsTFxVkVD4fxiYjILdTlpXf+/v5o3bq1xTpfX18EBweb1w8bNgzjx49HUFAQNBoNRo8ejfj4eHTu3BkA0Lt3b8TFxWHw4MF4++23kZOTgylTpiA5Obna0YTbYbEnIiK3IIoCRBuKvS37Vue9996DQqHAwIEDodPpkJCQgIULF5q3K5VKrFu3DiNHjkR8fDx8fX2RlJSEGTNmWP1eLPZERER1YNu2bRavvby8sGDBAixYsOCW+8TExGD9+vU2vzeLPRERuQU+z56IiEjm+CAcIiIiki327ImIyC042wS9usRiT0REbsGdh/FZ7ImIyC24c8+e5+yJiIhkjj17IiJyC6KNw/iu3LNnsSciIrcgAhBF2/Z3VRzGJyIikjn27ImIyC2YIEBw0zvosWdfA/2GXsane4/hh7N/4P11pxDbvlzqkGzmqjnpV5ThardcVH5QYl4n6kRUvleMq/3ycPWhPOheL4RYYLTYz5iug+6lAlx9KA9XH8+HfnEJRINzD8q56md0O8zJNcgxJ+D6bHxbFlfFYn8H3R69ghFTL2HF3HAkJzTH2WNemJV6FtpgvdSh1Zqr5mQ6rodxbTmEJpYDUvr/K4Fplw6e0wOgfj8Q4mUTKl8vur7faT0qJxVC8S9PqD8OgudULYy/6mBYUlrXKdSYq35Gt8OcXIMccyKJi/2iRYvQtm1baDQaaDQaxMfH46effpIypJsMGHEZG1KDsOmrIGSe8sL8SZHQXRWQMKhA6tBqzRVzEstNqHyzCB4TNRD8r/+6FktNMK6/Co9kfyjv8oQi1gOer2pgOqKH6WglAMD4cwWExip4DPWDIlIFZXtPeLzoD8PqcojlJqlSui1X/IzuhDm5BjnmVKXqpjq2LK5K0mIfGRmJOXPmID09Hfv370fPnj3Rv39/HD16VMqwzFQeJjRrW44DO/zN60RRwMEd/ojr6JrDWq6ak35eCRTxaijvVlusN500AAZA0dHTvE4Ro4IQpoDp6N89ET0geP7jH6kaQCVgyjA4OHLruepndDvMyTXIMacbiaLti6uStNj369cPDz/8MJo1a4bmzZtj1qxZ8PPzw549e6ptr9PpUFxcbLE4kibICKUKKMy3HDa+clmFwBDnKxI14Yo5GbZWwHTSAI/hfjdtE/8yAh6A4P+Pr3KgAmLBtV674l+eMB3Vw7DlKkSjCDHfCMOnZdf3dzKu+BndCXNyDXLMia5xmnP2RqMRK1euRFlZGeLj46ttk5KSAq1Wa16ioqLqOEqqa6Y8I/QflMDzdQ0Ede2G0JT3qKF60Q/6uSWoeDAPFc9ehqLz3yMECtcdliMi67jzBD3JL707fPgw4uPjUVFRAT8/P6xevRpxcXHVtp08eTLGjx9vfl1cXOzQgl9coITRAAT84xdtYD0DruRL/ldXK66Wk5ihB66YoBt+w/lCI4Df9bi6uhye/wsA9IBYYrLs3V8xQQi6/trjaV+onvIB/jIB/gqI2UYYlpRCUV9ZZ7nUlKt9RjXBnFyDHHO6Ee+NL6HY2FgcOnQIe/fuxciRI5GUlIRjx45V21atVpsn81UtjmTQK3DqDx906Hr9Mi9BENG+aymOpfs49L0dxdVyUnT0hHpZMNQfX1+EWBWUvbyg/jgYilgPQAWYDlSa9zFlGiDmmqBo5WFxLEEQINRTQlALMG6tgBCqgNDc+f4Dc7XPqCaYk2uQY043cucJepL/T+fp6YmmTZsCADp27Ih9+/bh/fffx4cffihxZNd8t6QeJszLwsnffZBx0AePD8+Hl48Jm1YGSR1arblSToKPAkJjy9+kgrcAaBVQNL729VU+7A39ghLAXwHBV4D+/RIoWnlA0er6pD39l2VQ/ssTUAgwbq+AIbUMntO0EJTO+Y/XlT6jmmJOrkGOOZETFPt/MplM0Ol0UodhlrY2ENpgI4ZMzEFgiAFnj3rjtcRGKLzsceednZTccvIY5Q+9Aqh8oxDQi1Dco4bnOH+LNqa9Ohi+KAMqRQhNPeA5KwDKzurqD+gE5PYZAczJVcgxpyq2zqh35dn4gihKF/7kyZPRp08fREdHo6SkBKmpqXjrrbewceNGPPjgg3fcv7i4GFqtFt3RHyrB9b+IcuadFiZ1CHZ1tVuu1CEQyYJB1GMbvkdRUZHDTs1W1YpmX7wKpY9XrY9jLK/AqWfnODRWR5G0Z5+Xl4chQ4YgOzsbWq0Wbdu2rXGhJyIiopqRtNgvXbpUyrcnIiI34s6z8Z3unD0REZEjiLDtmfQufMpe+kvviIiIyLHYsyciIrfAYXwiIiK5c+NxfBZ7IiJyD7be396Fe/Y8Z09ERCRz7NkTEZFbcOc76LHYExGRW3DnCXocxiciIpI59uyJiMg9iIJtk+xcuGfPYk9ERG7Bnc/ZcxifiIhI5tizJyIi98Cb6hAREcmbO8/Gr1GxX7t2bY0P+Oijj9Y6GCIiIrK/GhX7xx57rEYHEwQBRqPRlniIiIgcx4WH4m1Ro2JvMpkcHQcREZFDufMwvk2z8SsqKuwVBxERkWOJdlhclNUT9IxGI2bPno3FixcjNzcXJ0+eROPGjfH666+jYcOGGDZsmCPiJBd3tVuu1CHY1cxz+6QOwe7eaBYvdQh2JxoMUodA5BSs7tnPmjULy5cvx9tvvw1PT0/z+tatW+Pjjz+2a3BERET2I9hhcU1WF/vPPvsMS5YsQWJiIpRKpXl9u3btcOLECbsGR0REZDduPIxvdbG/ePEimjZtetN6k8kEvV5vl6CIiIjIfqwu9nFxcdixY8dN67/55ht06NDBLkERERHZnRv37K2eoPfGG28gKSkJFy9ehMlkwnfffYeMjAx89tlnWLdunSNiJCIisp0bP/XO6p59//798cMPP2DLli3w9fXFG2+8gePHj+OHH37Agw8+6IgYiYiIyAa1ujf+fffdh82bN9s7FiIiIodx50fc1vpBOPv378fx48cBXDuP37FjR7sFRUREZHd86l3NXbhwAYMGDcKvv/6KgIAAAEBhYSHuvfderFy5EpGRkfaOkYiIiGxg9Tn7F154AXq9HsePH0dBQQEKCgpw/PhxmEwmvPDCC46IkYiIyHZVE/RsWVyU1T37tLQ07Nq1C7GxseZ1sbGx+OCDD3DffffZNTgiIiJ7EcRriy37uyqri31UVFS1N88xGo2IiIiwS1BERER258bn7K0exv/f//6H0aNHY//+/eZ1+/fvx5gxY/DOO+/YNTgiIiJXtWjRIrRt2xYajQYajQbx8fH46aefzNsrKiqQnJyM4OBg+Pn5YeDAgcjNtXxoWGZmJvr27QsfHx+EhoZi4sSJMNTiAU816tkHBgZCEK6fqygrK0OnTp2gUl3b3WAwQKVS4fnnn8djjz1mdRBEREQOV8c31YmMjMScOXPQrFkziKKITz/9FP3798fBgwfRqlUrjBs3Dj/++CO+/vpraLVajBo1CgMGDMCvv/4K4NqIed++fREeHo5du3YhOzsbQ4YMgYeHB2bPnm1VLDUq9vPmzbPqoERERE7HTsP4xcXFFqvVajXUavVNzfv162fxetasWVi0aBH27NmDyMhILF26FKmpqejZsycAYNmyZWjZsiX27NmDzp07Y9OmTTh27Bi2bNmCsLAwtG/fHjNnzsSkSZMwbdo0iyfP3kmNin1SUlKND0hERCRnUVFRFq+nTp2KadOm3XYfo9GIr7/+GmVlZYiPj0d6ejr0ej169eplbtOiRQtER0dj9+7d6Ny5M3bv3o02bdogLCzM3CYhIQEjR47E0aNHrXoeTa1vqgNcO99QWVlpsU6j0dhySCIiIsewU88+KyvLotZV16uvcvjwYcTHx6OiogJ+fn5YvXo14uLicOjQIXh6eprvV1MlLCwMOTk5AICcnByLQl+1vWqbNawu9mVlZZg0aRJWrVqFv/7666btRqPR2kMSERE5np2KfdWEu5qIjY3FoUOHUFRUhG+++QZJSUlIS0uzIYjasXo2/iuvvIKff/4ZixYtglqtxscff4zp06cjIiICn332mSNiJCIickmenp5o2rQpOnbsiJSUFLRr1w7vv/8+wsPDUVlZicLCQov2ubm5CA8PBwCEh4ffNDu/6nVVm5qyutj/8MMPWLhwIQYOHAiVSoX77rsPU6ZMwezZs7FixQprD0dERFQ3nOAOeiaTCTqdDh07doSHhwe2bt1q3paRkYHMzEzEx8cDAOLj43H48GHk5eWZ22zevBkajQZxcXFWva/Vw/gFBQVo3LgxgGtDGQUFBQCArl27YuTIkdYejoiIqE7U9R30Jk+ejD59+iA6OholJSVITU3Ftm3bsHHjRmi1WgwbNgzjx49HUFAQNBoNRo8ejfj4eHTu3BkA0Lt3b8TFxWHw4MF4++23kZOTgylTpiA5Ofm28wSqY3Wxb9y4Mc6dO4fo6Gi0aNECq1atwr/+9S/88MMPN000kIt+Qy/jiZF5CAox4Owxbyyc0gAZh3ykDssmzEk6P8+LwC/vN7BYV6/xVYzZegQAsC81BH+sDUL2UV/oSpX47+8H4K2xnAtz6YgPNs2JxMU/fCEogVYPFeChKVlQ+5rqLA9rKBQinh13CT0fL0BgqB5/5Xpgy9f1kDo/HIDr3m8ccJ3vnTXkmJMU8vLyMGTIEGRnZ0Or1aJt27bYuHEjHnzwQQDAe++9B4VCgYEDB0Kn0yEhIQELFy40769UKrFu3TqMHDkS8fHx8PX1RVJSEmbMmGF1LFYP4z/33HP4/fffAQCvvvoqFixYAC8vL4wbNw4TJ060OoAqc+bMgSAIGDt2bK2P4QjdHr2CEVMvYcXccCQnNMfZY16YlXoW2uCbbxnsKpiT9EKbl+OV3w6alxe+PmHepq9QoFm3Itz/0qVq9y3O9cDyZ2MR1FCHEauPY8jyk8g75Y3vJjSqq/Ct9uTIHPQdnI+Fb0RjRM9W+CQlEk+8mIP+z+VLHZpNXO17VxNyzMlMtMNihaVLl+L8+fPQ6XTIy8vDli1bzIUeALy8vLBgwQIUFBSgrKwM33333U3n4mNiYrB+/XqUl5cjPz8f77zzjvmGdtawutiPGzcOL7/8MgCgV69eOHHiBFJTU3Hw4EGMGTPG6gAAYN++ffjwww/Rtm3bWu3vSANGXMaG1CBs+ioImae8MH9SJHRXBSQMKpA6tFpjTtJTKAH/EIN58Q26fvvLe5/Pxf0jcxDVoazafTO2BkChEvHIjD8R0qQCke3K0O/NP3FsQxD+Om/d0F5dibu7DHs2BeC3n7XIvaDGzvWBOLBdg9h21efoKlzte1cTcsyJalHs/ykmJgYDBgyodaEuLS1FYmIiPvroIwQGBtoajl2pPExo1rYcB3b4m9eJooCDO/wR17Fcwshqjzk5h7/Oq/F2p3aYe38bfD22MQov1vxOWMZKAUpPEYob/vV6eF0bvv9zv5+9Q7WLY/t90b5LCRo0qgAANGpZjlb3lGLfNte9L4crfu/uRI453UjA9fP2tVqkTsAGNRoLmD9/fo0PWNXrr6nk5GT07dsXvXr1wptvvnnbtjqdDjqdzvz6n7cstDdNkBFKFVCYb/nXdOWyClFNdbfYy7kxJ+lFti/DgP+dQ73GFSjJ88Av8xvg46daYPTGI1D73fmce6N7i/HTrCjs/DAcnZ/Lhf6qApveigQAlOTV/EdDXVq1MBw+/kZ89MtRmIzXRjY+/V8EflkTLHVoteZq37uakGNOdE2Niv17771Xo4MJgmBVsV+5ciUOHDiAffv21ah9SkoKpk+fXuPjEzmj5t2LzH8Ob3kVkR3K8G7XtjjyYxA6Pn35jvuHNa/AgHfOYcOb0dj8v0gIShGdk3LhV08PQeGcz+C8/5Er6PlYAd4a3Qh/nvRGk1bl+H9Ts/BXrie2fOO6BZ9cTB0/CMeZ1KjYnzt3zu5vnJWVhTFjxmDz5s3w8vKq0T6TJ0/G+PHjza+Li4tvukexPRUXKGE0AAEhlo8TDKxnwJV8m+40LBnm5Hy8NUbUa6TDX3/W7N8BALTrX4B2/QtQmq+Ch48JggDsWhqOoCjn7H298NoFrFoYjrQfggAA5zO8EdqgEk+/lO2yxd7Vv3fVkWNOFvg8+7qXnp6OvLw83HXXXVCpVFCpVEhLS8P8+fOhUqmqve2uWq0236bQmtsV1pZBr8CpP3zQoWuJeZ0giGjftRTH0l3zMhTm5Hx0ZQoU/KmGf0jlnRv/g1+IAWpfEw6vC4JKbUKT+xx7aqu21N4mmEyWvSKTCRAk+x/Idq7+vauOHHOiayT7qfbAAw/g8OHDFuuee+45tGjRApMmTYJSqZQoMkvfLamHCfOycPJ3H2Qc9MHjw/Ph5WPCppVBUodWa8xJWhtmRSH2gUIEROpQkuuJn9+LgKAU0fbRa7OdS/JVKM33MM+szz3hDbWfEdqISvgEXPsRvOfTUER3LIWnjxFndmqxMSUSD75y4abr8Z3F3i0BeGZ0NvIveeLPk15o0qocj7+Qh02rXLNXX8WVvnc1JceczNy4Zy9Zsff390fr1q0t1vn6+iI4OPim9VJKWxsIbbARQybmIDDEgLNHvfFaYiMUXvaQOrRaY07SKsrxwNdjGqO8UAXfIAOi7y7B//vuOHyDrw2d7lsRanHTnaVPtwQAPP6/s7jriWsPn7rwuy9+ntcAleUK1GtcgUdn/Yn2A25+MJWzWPhGFIZMuITkNzMRUO/aTXV+WlEPK96vL3VoNnGl711NyTGnKnV9Bz1nIoii6DThd+/eHe3bt8e8efNq1L64uBharRbd0R8qwfW/iOQ6Zp6r2aRSV/JGs3ipQ7A70WC4cyOSlEHUYxu+R1FRkcNOzVbVioazZkFRwzli1TFVVOD8a685NFZHcaoZF9u2bZM6BCIikis3Hsav1fSYHTt24Nlnn0V8fDwuXrwIAPj888+xc+dOuwZHRERkN3V8u1xnYnWx//bbb5GQkABvb28cPHjQfJOboqIizJ492+4BEhERkW2sLvZvvvkmFi9ejI8++ggeHtfPk3fp0gUHDhywa3BERET2YtOtcm2c3Cc1q8/ZZ2Rk4P77779pvVarRWFhoT1iIiIisj83voOe1T378PBwnD59+qb1O3fuROPGje0SFBERkd3xnH3NDR8+HGPGjMHevXshCAIuXbqEFStWYMKECRg5cqQjYiQiIiIbWD2M/+qrr8JkMuGBBx5AeXk57r//fqjVakyYMAGjR492RIxEREQ2c+eb6lhd7AVBwGuvvYaJEyfi9OnTKC0tRVxcHPz8nPM52kRERADc+jr7Wt9Ux9PTE3FxcfaMhYiIiBzA6mLfo0cPCMKtZyT+/PPPNgVERETkELZePudOPfv27dtbvNbr9Th06BCOHDmCpKQke8VFRERkXxzGr7n33nuv2vXTpk1DaWmpzQERERGRfdXq3vjVefbZZ/HJJ5/Y63BERET25cbX2dvtqXe7d++Glw2PDiQiInIkXnpnhQEDBli8FkUR2dnZ2L9/P15//XW7BUZERET2YXWx12q1Fq8VCgViY2MxY8YM9O7d226BERERkX1YVeyNRiOee+45tGnTBoGBgY6KiYiIyP7ceDa+VRP0lEolevfuzafbERGRy3HnR9xaPRu/devWOHv2rCNiISIiIgew+pz9m2++iQkTJmDmzJno2LEjfH19LbZrNBq7BUfkrF5v0lnqEOxu44X9UodgdwkR7aUOgZyNC/fObVHjYj9jxgz85z//wcMPPwwAePTRRy1umyuKIgRBgNFotH+UREREtnLjc/Y1LvbTp0/Hiy++iF9++cWR8RAREZGd1bjYi+K1nzTdunVzWDBERESOwpvq1NDtnnZHRETk1DiMXzPNmze/Y8EvKCiwKSAiIiKyL6uK/fTp02+6gx4REZEr4DB+DT3zzDMIDQ11VCxERESO48bD+DW+qQ7P1xMREbkmq2fjExERuSQ37tnXuNibTCZHxkFERORQPGdPREQkd27cs7f6QThERETkWtizJyIi9+DGPXsWeyIicgvufM6ew/hEREQyx549ERG5Bw7jExERyRuH8YmIiEi22LMnIiL3wGF8IiIimXPjYs9hfCIiIpljz74G+g29jCdG5iEoxICzx7yxcEoDZBzykTosmzAn5/bs+EsYPD7HYl3WaTVe6N5Koohu7/N3wvHF3HCLdZFNKrB0xwnz62P7fbD8rfo4ccAHSiXQuNVVzE49A7W3iN93+eGVJ5pWe+z56zMQ2/6qQ+O3hZy+d1XkmBMACH8vtuzvqiQt9tOmTcP06dMt1sXGxuLEiRO32KPudXv0CkZMvYQPXo3EiQM+eHx4PmalnsWw+2JR9JeH1OHVCnNyDedPeOHVQc3Mr40G5/6vJib2KuZ8dcb8Wqm8PuZ5bL8PXktsgmdG5eKlNy9CqRRx9pg3hL/HFuPuLsOXh45YHO/Tt+vj0E4/NG/nvIVejt87OeZkxmF86bRq1QrZ2dnmZefOnVKHZGHAiMvYkBqETV8FIfOUF+ZPioTuqoCEQQVSh1ZrzMk1GI0CruR7mJfiK849EKdUAkGhBvOiDTaat304rQEeG5aPp0fnoWFsBaKa6tDt0UJ4qq/97+nhKVrsqwk0YPdGDXo/XQDBiX/jyPF7J8ecqlRdemfL4qokL/YqlQrh4eHmpV69elKHZKbyMKFZ23Ic2OFvXieKAg7u8Edcx3IJI6s95uQ6GjTSIXX/YSz/9QgmfXAOIRGVUod0WxfPeWJQh1ZI6twSc5KjkXfhWi+w8LIKJw74IiDYgLH9muHptq0wYUBTHNnre8tj7d6kRckVFXo/7bwFRo7fOznmJKWUlBTcc8898Pf3R2hoKB577DFkZGRYtKmoqEBycjKCg4Ph5+eHgQMHIjc316JNZmYm+vbtCx8fH4SGhmLixIkwGAxWxSJ5sT916hQiIiLQuHFjJCYmIjMz85ZtdTodiouLLRZH0gQZoVQBhfmWPaorl1UIDLHuL9pZMCfXcOKgL94ZF4PXBjfBB/+NRnhUJd797iS8fY133lkCLe4qw4R5mZi14gxGz7mAnEw1/vN4M5SXKpD9pycA4PO54eiT+BdmrTiLpm3K8erTTXDxrGe1x9v4ZTA6di9BSIS+LtOwihy/d3LMyYJoh8UKaWlpSE5Oxp49e7B582bo9Xr07t0bZWVl5jbjxo3DDz/8gK+//hppaWm4dOkSBgwYYN5uNBrRt29fVFZWYteuXfj000+xfPlyvPHGG1bFIum4YKdOnbB8+XLExsYiOzsb06dPx3333YcjR47A39//pvYpKSk3neMnkqP9v2jNfz53HDhx0Aef7zmC+/tdwcaVzjP6VeWeniXmPzeOq0CLDuUY/K84bF8bgKhmFQCAh5/9CwnPXOupN21zFYd2+mPjymA8/99si2PlX/JA+jZ//PfD83UWP7kROwzF/7OjqVaroVarb2q3YcMGi9fLly9HaGgo0tPTcf/996OoqAhLly5FamoqevbsCQBYtmwZWrZsiT179qBz587YtGkTjh07hi1btiAsLAzt27fHzJkzMWnSJEybNg2entX/YP4nSXv2ffr0wZNPPom2bdsiISEB69evR2FhIVatWlVt+8mTJ6OoqMi8ZGVlOTS+4gIljAYg4B+/aAPrGXAl37nPn94Kc3JNZcUqXDjrhYiGOqlDqRE/rRGRjXW4dF6N4LBrn0tM8wqLNlFNK5B38eYJX5u+CoJ/oAHxvYvqJNbakuP3To45OUJUVBS0Wq15SUlJqdF+RUXXvtNBQUEAgPT0dOj1evTq1cvcpkWLFoiOjsbu3bsBALt370abNm0QFhZmbpOQkIDi4mIcPXq0xjFLPox/o4CAADRv3hynT5+udrtarYZGo7FYHMmgV+DUHz7o0PV6r0UQRLTvWopj6a55GQpzck1ePkZENNShIM81ZkNfLVPg0p+eCArVIyyqEsHhlbhwxrLnc/GsGqGRlsP0onit2Pd64gpUTp6qHL93cszpRvaaoJeVlWXR8Zw8efId39tkMmHs2LHo0qULWrduDQDIycmBp6cnAgICLNqGhYUhJyfH3ObGQl+1vWpbTTnVT7XS0lKcOXMGgwcPljoUs++W1MOEeVk4+bsPMg5euwzFy8eETSuDpA6t1piT8xs+5QL2bNEi74IngsP0GPyfbBiNAratCZQ6tGotmR6Bzr2LEBqpx185Knz+Tn0oFUD3x69AEIAnRubj83fC0TjuKhq3uootXwch64wXpnx03uI4h3b6ISdTjYf+/Zc0iVhJbt87QJ45mdnp0rvadDaTk5Nx5MgRya44k7TYT5gwAf369UNMTAwuXbqEqVOnQqlUYtCgQVKGZSFtbSC0wUYMmZiDwBADzh71xmuJjVB42cm7HbfBnJxfvfp6TP6/8/APNKCoQIWjv/lh7KOxKCpwznwuZ3sg5aWGKLmihDbYgFb3lGHeupMI+PvyuwHD86GvELB4agOUFCrROK4CKV+eQURDyysMNnwZjLi7SxHdzDVOV8jtewfIMyepjRo1CuvWrcP27dsRGRlpXh8eHo7KykoUFhZa9O5zc3MRHh5ubvPbb79ZHK9qtn5Vm5oQRFGU7MrBZ555Btu3b8dff/2FkJAQdO3aFbNmzUKTJk1qtH9xcTG0Wi26oz9UAr+IVIcUSqkjsLuNF9KlDsHuEiLaSx0C3YFB1GMbvkdRUZHDTs1W1Yo2L8yG0tOr1scxVlbg8Mf/rXGsoihi9OjRWL16NbZt24ZmzZpZbC8qKkJISAi+/PJLDBw4EACQkZGBFi1aYPfu3ejcuTN++uknPPLII8jOzkZoaCgAYMmSJZg4cSLy8vKqnRhYHUl79itXrpTy7YmIyJ3U8R30kpOTkZqaiu+//x7+/v7mc+xarRbe3t7QarUYNmwYxo8fj6CgIGg0GowePRrx8fHo3LkzAKB3796Ii4vD4MGD8fbbbyMnJwdTpkxBcnJyjQs94GTn7ImIiORi0aJFAIDu3btbrF+2bBmGDh0KAHjvvfegUCgwcOBA6HQ6JCQkYOHChea2SqUS69atw8iRIxEfHw9fX18kJSVhxowZVsXCYk9ERG7B1lveWrtvTc6Se3l5YcGCBViwYMEt28TExGD9+vXWvfk/sNgTEZF7cOMH4bDYExGRe3DjYu9UN9UhIiIi+2PPnoiI3EJdn7N3Jiz2RETkHjiMT0RERHLFnj0REbkFQRQh2HDTWFv2lRqLPRERuQcO4xMREZFcsWdPRERugbPxiYiI5I7D+ERERCRX7NkTEZFb4DA+ERGR3LnxMD6LPRERuQV37tnznD0REZHMsWdPRETugcP4RGQVk1HqCOwuIaK91CHY3cqsXVKHYHfPRN0rdQguzZWH4m3BYXwiIiKZY8+eiIjcgyheW2zZ30Wx2BMRkVvgbHwiIiKSLfbsiYjIPXA2PhERkbwJpmuLLfu7Kg7jExERyRx79kRE5B44jE9ERCRv7jwbn8WeiIjcgxtfZ89z9kRERDLHnj0REbkFDuMTERHJnRtP0OMwPhERkcyxZ09ERG6Bw/hERERyx9n4REREJFfs2RMRkVvgMD4REZHccTY+ERERyRV79jXQb+hlPDEyD0EhBpw95o2FUxog45CP1GHZhDk5P7nlA7hOTl/PjcK370VZrItoUo652w4BAKY/2QrH92gttvd6NgcvpJw1v7580RNL/9sER3dp4OVrwv1P5GHQq39C6QL/67rK52Qtdx7GZ8/+Dro9egUjpl7CirnhSE5ojrPHvDAr9Sy0wXqpQ6s15uT85JYP4Ho5RTYvx+L0feZl2ndHLLb3/HeOxfZ///dP8zaTEXgrqSUMegEz1hzGyPdOIe3rUKx6J7qu07Caq31OVjGJti8uSvJif/HiRTz77LMIDg6Gt7c32rRpg/3790sdltmAEZexITUIm74KQuYpL8yfFAndVQEJgwqkDq3WmJPzk1s+gOvlpFSJCAjVmxdNkMFiu9rbZLHdx99o3vb79gBcOOWD5PdPoWGrcnToUYinJmRi02fhMFQKdZ2KVVztc7KKaIfFRUla7K9cuYIuXbrAw8MDP/30E44dO4Z3330XgYGBUoZlpvIwoVnbchzY4W9eJ4oCDu7wR1zHcgkjqz3m5Pzklg/gmjnlnPPCyI534+Uud+GD0c1w+aKnxfadq0MwvO09mPBAe3w5Jxq6q9f/Oz2V7o/oFuUICLneG27XrRBXS1TIOum8w+Gu+DlRzUh69uitt95CVFQUli1bZl7XqFEjCSOypAkyQqkCCvMt/5quXFYhqqlOoqhsw5ycn9zyAVwvp6YdSjBy7mnUb3IVhbme+GZeJKYNbIP/bTkIbz8Tujx2GSENdAgMq0TmCR+kzo7BpTPe+M9HGQCAwnwPaOtVWhxT+3fhL8z3qPN8asrVPidrCbDxnL3dIql7khb7tWvXIiEhAU8++STS0tLQoEEDvPTSSxg+fHi17XU6HXS661+44uLiugqViNxIhx6F5j/HtCxH0w4lGBXfEbvX1UPPZ/LQKzHXvD26ZTkCQivx5jOtkXNejfCGrl8UZYt30JPG2bNnsWjRIjRr1gwbN27EyJEj8fLLL+PTTz+ttn1KSgq0Wq15iYqKqradvRQXKGE0AAEhlufqAusZcCXfBabUVoM5OT+55QO4fk6+WiPqN6pA7nmvarc37VAKAMg97w0ACAjRo+iy5bB/0d89+huH9p2Nq39OdGuSFnuTyYS77roLs2fPRocOHTBixAgMHz4cixcvrrb95MmTUVRUZF6ysrIcGp9Br8CpP3zQoWuJeZ0giGjftRTH0p33vNvtMCfnJ7d8ANfPqaJMgdw/1QgIrax2+59HfQEAAWHXtjfrWILMEz4ounx9yP6PHQHw9jcgspnznvt29c/pTqouvbNlcVWS/lSrX78+4uLiLNa1bNkS3377bbXt1Wo11Gp1XYRm9t2SepgwLwsnf/dBxkEfPD48H14+JmxaGVSncdgTc3J+cssHcK2cPp8Zg469rqBepA5Xcj3xzdwoKJRAl/6XkXNejV/XhKBDzyvwCzQg87gPPpveCC07FSGm5bVC3u7+QkQ2K8eCMU2R+NqfKMzzxKr/RaP3kBx4qJ27YrjS52Q1N76DnqTFvkuXLsjIyLBYd/LkScTExEgU0c3S1gZCG2zEkIk5CAwx4OxRb7yW2AiFl513ks2dMCfnJ7d8ANfKqSBbjQ9GNUdJoQqaID1i7ynBzO//gCbYgEqdAkd2avHT0vrQXVUiuL4OnR7+C4+/fMG8v0IJvLL8BJb+tzFe798Gap9rN9V5akKmhFnVjCt9TlRzgihKN+Ng3759uPfeezF9+nQ89dRT+O233zB8+HAsWbIEiYmJd9y/uLgYWq0W3dEfKoFfRCKytDJrl9Qh2N0zUfdKHYJdGUQ9tuF7FBUVQaPROOQ9qmrFfd2nQqWqft5FTRgMFdixbbpDY3UUSc/Z33PPPVi9ejW+/PJLtG7dGjNnzsS8efNqVOiJiIisYrLD4qIkn175yCOP4JFHHpE6DCIiItmSvNgTERHVBUEUIdhw5tqWfaXGYk9ERO6Bs/GJiIhkjnfQIyIiInvavn07+vXrh4iICAiCgDVr1lhsF0URb7zxBurXrw9vb2/06tULp06dsmhTUFCAxMREaDQaBAQEYNiwYSgtLbU6FhZ7IiJyC3V9B72ysjK0a9cOCxYsqHb722+/jfnz52Px4sXYu3cvfH19kZCQgIqKCnObxMREHD16FJs3b8a6deuwfft2jBgxwurcOYxPRETuoY6H8fv06YM+ffrc4lAi5s2bhylTpqB///4AgM8++wxhYWFYs2YNnnnmGRw/fhwbNmzAvn37cPfddwMAPvjgAzz88MN45513EBERUeNY2LMnIiKyQnFxscVy49NYa+rcuXPIyclBr169zOu0Wi06deqE3bt3AwB2796NgIAAc6EHgF69ekGhUGDv3r1WvR+LPRERuQXBZPsCAFFRURZPYE1JSbE6lpycHABAWFiYxfqwsDDztpycHISGhlpsV6lUCAoKMrepKQ7jExGRe7DTMH5WVpbF7XLr+gFttcGePRERkRU0Go3FUptiHx4eDgDIzc21WJ+bm2veFh4ejry8PIvtBoMBBQUF5jY1xWJPRETuQbTDYieNGjVCeHg4tm7dal5XXFyMvXv3Ij4+HgAQHx+PwsJCpKenm9v8/PPPMJlM6NSpk1Xvx2F8IiJyC3V9u9zS0lKcPn3a/PrcuXM4dOgQgoKCEB0djbFjx+LNN99Es2bN0KhRI7z++uuIiIjAY489BgBo2bIlHnroIQwfPhyLFy+GXq/HqFGj8Mwzz1g1Ex9gsSciInKI/fv3o0ePHubX48ePBwAkJSVh+fLleOWVV1BWVoYRI0agsLAQXbt2xYYNG+Dldf0xvCtWrMCoUaPwwAMPQKFQYODAgZg/f77VsbDYExGRe6jj6+y7d+8O8Tb7CIKAGTNmYMaMGbdsExQUhNTUVKvetzos9kRE5B5E2PZMete9NT6LPRERuQd3fsQtZ+MTERHJHHv2RETkHkTYeM7ebpHUORZ7IiJyD278PHsWeyKSrWei7pU6BLsrH2DdzVScnUFfAaz9XuowZI/FnoiI3IMJgGDj/i6KxZ6IiNwCZ+MTERGRbLFnT0RE7oET9IiIiGTOjYs9h/GJiIhkjj17IiJyD27cs2exJyIi98BL74iIiOSNl94RERGRbLFnT0RE7oHn7ImIiGTOJAKCDQXb5LrFnsP4REREMseePRERuQcO4xMREcmdjcUerlvsOYxPREQkc+zZExGRe+AwPhERkcyZRNg0FM/Z+EREROSs2LMnIiL3IJquLbbs76LYs6+BfkMv49O9x/DD2T/w/rpTiG1fLnVINmNOzk9u+QDMSWrtmmRjzv/bgNWzvsCO/1uC+9qev2Xb/zyzAzv+bwme7H642u0eKiM+efVb7Pi/JWja4LKDIrazqnP2tiwuisX+Dro9egUjpl7CirnhSE5ojrPHvDAr9Sy0wXqpQ6s15uT85JYPwJycgZdaj9MXgzH3qy63bXdf23No1TAP+YU+t2wzsv9eXC669XanZBJtX1yUpMW+YcOGEAThpiU5OVnKsCwMGHEZG1KDsOmrIGSe8sL8SZHQXRWQMKhA6tBqjTk5P7nlAzAnZ7D3WDQ+XncPdvzR6JZt6mnLMPbJXZixvAcMxupLRKe4TNzT8gIWru7sqFDJziQt9vv27UN2drZ52bx5MwDgySeflDIsM5WHCc3aluPADn/zOlEUcHCHP+I6Ou9Q3e0wJ+cnt3wA5uQqBEHElCG/4MutbXE+J6jaNoH+5Xhl0A68+VkPVFS62LQvDuNLIyQkBOHh4eZl3bp1aNKkCbp161Zte51Oh+LiYovFkTRBRihVQGG+5Rf6ymUVAkMMDn1vR2FOzk9u+QDMyVUkPngIRpOAb7a1vkULEf99Ng3f72yJjMyQOo3NLkTYWOylTqD2nOacfWVlJb744gs8//zzEASh2jYpKSnQarXmJSoqqo6jJCKSp+ZR+Xii+xHM/qI7gOr/Dx7Y7Sh8vCrxxab2dRka2YHTjMGsWbMGhYWFGDp06C3bTJ48GePHjze/Li4udmjBLy5QwmgAAv7xKz2wngFX8p3mr84qzMn5yS0fgDm5gnZNchDodxXfzEg1r1MpRSQP2IMnexzGU1P/jY7NL6JVozxsnbfUYt+PXlmNzfubYvbnPeo6bOvwDnrSW7p0Kfr06YOIiIhbtlGr1VCr1XUWk0GvwKk/fNChawl2b9ACuHZOq33XUqxdHlxncdgTc3J+cssHYE6uYOO+Ztif0cBi3bvJ67Hxt2ZYvycWADDvmy74aN095u31tOWYO2o9pi17AMfOh9ZpvLViMgGw4Vp5k+teZ+8Uxf7PP//Eli1b8N1330kdyk2+W1IPE+Zl4eTvPsg46IPHh+fDy8eETSurn7ziCpiT85NbPgBzcgbenno0CCkyv64fXIymDS6juNwLeVf8UFzmZdHeYFSgoNgHWXkBAIC8K34W26/qPAAAF/M1yC+03EbOxSmK/bJlyxAaGoq+fftKHcpN0tYGQhtsxJCJOQgMMeDsUW+8ltgIhZc9pA6t1piT85NbPgBzcgaxMfn4YMw68+vRA/cAAH7a0/zvc/Uy58bD+IIoShu9yWRCo0aNMGjQIMyZM8eqfYuLi6HVatEd/aESnPMfFxGRPZUP6CR1CHZl0Ffgt7Wvo6ioCBqNxiHvUVUretV7HiqFZ62PYzBVYsvlTxwaq6NIPht/y5YtyMzMxPPPPy91KERERLIk+TB+7969IfHgAhERuQM3fsSt5MWeiIioLoiiCaINT66zZV+psdgTEZF7EG18mI0Lj0JLfs6eiIiIHIs9eyIicg+ijefsXbhnz2JPRETuwWQCBBvOu7vwOXsO4xMREckce/ZEROQeOIxPREQkb6LJBNGGYXxXvvSOw/hEREQyx549ERG5Bw7jExERyZxJBAT3LPYcxiciIpI59uyJiMg9iCIAW66zd92ePYs9ERG5BdEkQrRhGN+Vn9DKYk9ERO5BNMG2nj0vvSMiIqJqLFiwAA0bNoSXlxc6deqE3377rc5jYLEnIiK3IJpEmxdrffXVVxg/fjymTp2KAwcOoF27dkhISEBeXp4DMrw1FnsiInIPosn2xUpz587F8OHD8dxzzyEuLg6LFy+Gj48PPvnkEwckeGsufc6+arKEAXqb7pNAROQqDPoKqUOwK+Pf+dTF5Ddba4UBegBAcXGxxXq1Wg21Wn1T+8rKSqSnp2Py5MnmdQqFAr169cLu3btrH0gtuHSxLykpAQDsxHqJIyEiqiNrv5c6AocoKSmBVqt1yLE9PT0RHh6OnTm21wo/Pz9ERUVZrJs6dSqmTZt2U9vLly/DaDQiLCzMYn1YWBhOnDhhcyzWcOliHxERgaysLPj7+0MQBIe+V3FxMaKiopCVlQWNRuPQ96oLcssHYE6ugjk5v7rMRxRFlJSUICIiwmHv4eXlhXPnzqGystLmY4mieFO9qa5X72xcutgrFApERkbW6XtqNBpZ/GOuIrd8AObkKpiT86urfBzVo7+Rl5cXvLy8HP4+N6pXrx6USiVyc3Mt1ufm5iI8PLxOY+EEPSIiIgfw9PREx44dsXXrVvM6k8mErVu3Ij4+vk5jcemePRERkTMbP348kpKScPfdd+Nf//oX5s2bh7KyMjz33HN1GgeLfQ2p1WpMnTrVJc7N1ITc8gGYk6tgTs5PbvlI6emnn0Z+fj7eeOMN5OTkoH379tiwYcNNk/YcTRBd+Wa/REREdEc8Z09ERCRzLPZEREQyx2JPREQkcyz2REREMsdifwfbt29Hv379EBERAUEQsGbNGqlDsklKSgruuece+Pv7IzQ0FI899hgyMjKkDssmixYtQtu2bc03AImPj8dPP/0kdVh2M2fOHAiCgLFjx0odSq1NmzYNgiBYLC1atJA6LJtdvHgRzz77LIKDg+Ht7Y02bdpg//79UodVaw0bNrzpcxIEAcnJyVKHRjZisb+DsrIytGvXDgsWLJA6FLtIS0tDcnIy9uzZg82bN0Ov16N3794oKyuTOrRai4yMxJw5c5Ceno79+/ejZ8+e6N+/P44ePSp1aDbbt28fPvzwQ7Rt21bqUGzWqlUrZGdnm5edO3dKHZJNrly5gi5dusDDwwM//fQTjh07hnfffReBgYFSh1Zr+/bts/iMNm/eDAB48sknJY6MbMXr7O+gT58+6NOnj9Rh2M2GDRssXi9fvhyhoaFIT0/H/fffL1FUtunXr5/F61mzZmHRokXYs2cPWrVqJVFUtistLUViYiI++ugjvPnmm1KHYzOVSlXntwh1pLfeegtRUVFYtmyZeV2jRo0kjMh2ISEhFq/nzJmDJk2aoFu3bhJFRPbCnr2bKyoqAgAEBQVJHIl9GI1GrFy5EmVlZXV+O0p7S05ORt++fdGrVy+pQ7GLU6dOISIiAo0bN0ZiYiIyMzOlDskma9euxd13340nn3wSoaGh6NChAz766COpw7KbyspKfPHFF3j++ecd/qAxcjz27N2YyWTC2LFj0aVLF7Ru3VrqcGxy+PBhxMfHo6KiAn5+fli9ejXi4uKkDqvWVq5ciQMHDmDfvn1Sh2IXnTp1wvLlyxEbG4vs7GxMnz4d9913H44cOQJ/f3+pw6uVs2fPYtGiRRg/fjz++9//Yt++fXj55Zfh6emJpKQkqcOz2Zo1a1BYWIihQ4dKHQrZAYu9G0tOTsaRI0dc/twpAMTGxuLQoUMoKirCN998g6SkJKSlpblkwc/KysKYMWOwefPmOn9Kl6PceCqsbdu26NSpE2JiYrBq1SoMGzZMwshqz2Qy4e6778bs2bMBAB06dMCRI0ewePFiWRT7pUuXok+fPg599CzVHQ7ju6lRo0Zh3bp1+OWXX+r8McGO4OnpiaZNm6Jjx45ISUlBu3bt8P7770sdVq2kp6cjLy8Pd911F1QqFVQqFdLS0jB//nyoVCoYjUapQ7RZQEAAmjdvjtOnT0sdSq3Vr1//ph+TLVu2dPnTEwDw559/YsuWLXjhhRekDoXshD17NyOKIkaPHo3Vq1dj27ZtLj+h6FZMJhN0Op3UYdTKAw88gMOHD1use+6559CiRQtMmjQJSqVSosjsp7S0FGfOnMHgwYOlDqXWunTpctNlqydPnkRMTIxEEdnPsmXLEBoair59+0odCtkJi/0dlJaWWvQ+zp07h0OHDiEoKAjR0dESRlY7ycnJSE1Nxffffw9/f3/k5OQAALRaLby9vSWOrnYmT56MPn36IDo6GiUlJUhNTcW2bduwceNGqUOrFX9//5vmUPj6+iI4ONhl51ZMmDAB/fr1Q0xMDC5duoSpU6dCqVRi0KBBUodWa+PGjcO9996L2bNn46mnnsJvv/2GJUuWYMmSJVKHZhOTyYRly5YhKSkJKhVLhGyIdFu//PKLCOCmJSkpSerQaqW6XACIy5Ytkzq0Wnv++efFmJgY0dPTUwwJCREfeOABcdOmTVKHZVfdunUTx4wZI3UYtfb000+L9evXFz09PcUGDRqITz/9tHj69Gmpw7LZDz/8ILZu3VpUq9ViixYtxCVLlkgdks02btwoAhAzMjKkDoXsiI+4JSIikjlO0CMiIpI5FnsiIiKZY7EnIiKSORZ7IiIimWOxJyIikjkWeyIiIpljsSciIpI5FnsiIiKZY7EnstHQoUPx2GOPmV93794dY8eOrfM4tm3bBkEQUFhYeMs2giBgzZo1NT7mtGnT0L59e5viOn/+PARBwKFDh2w6DhHVHos9ydLQoUMhCAIEQTA/EW/GjBkwGAwOf+/vvvsOM2fOrFHbmhRoIiJb8SkHJFsPPfQQli1bBp1Oh/Xr1yM5ORkeHh6YPHnyTW0rKyvh6elpl/cNCgqyy3GIiOyFPXuSLbVajfDwcMTExGDkyJHo1asX1q5dC+D60PusWbMQERGB2NhYAEBWVhaeeuopBAQEICgoCP3798f58+fNxzQajRg/fjwCAgIQHByMV155Bf98vMQ/h/F1Oh0mTZqEqKgoqNVqNG3aFEuXLsX58+fRo0cPAEBgYCAEQcDQoUMBXHvyWEpKCho1agRvb2+0a9cO33zzjcX7rF+/Hs2bN4e3tzd69OhhEWdNTZo0Cc2bN4ePjw8aN26M119/HXq9/qZ2H374IaKiouDj44OnnnoKRUVFFts//vhjtGzZEl5eXmjRogUWLlxodSxE5Dgs9uQ2vL29UVlZaX69detWZGRkYPPmzVi3bh30ej0SEhLg7++PHTt24Ndff4Wfnx8eeugh837vvvsuli9fjk8++QQ7d+5EQUEBVq9efdv3HTJkCL788kvMnz8fx48fx4cffgg/Pz9ERUXh22+/BQBkZGQgOzsb77//PgAgJSUFn332GRYvXoyjR49i3LhxePbZZ5GWlgbg2o+SAQMGoF+/fjh06BBeeOEFvPrqq1b/nfj7+2P58uU4duwY3n//fXz00Ud47733LNqcPn0aq1atwg8//IANGzbg4MGDeOmll8zbV6xYgTfeeAOzZs3C8ePHMXv2bLz++uv49NNPrY6HiBxE4qfuETlEUlKS2L9/f1EURdFkMombN28W1Wq1OGHCBPP2sLAwUafTmff5/PPPxdjYWNFkMpnX6XQ60dvbW9y4caMoiqJYv3598e233zZv1+v1YmRkpPm9RNHycbQZGRkiAHHz5s3Vxln1COUrV66Y11VUVIg+Pj7irl27LNoOGzZMHDRokCiKojh58mQxLi7OYvukSZNuOtY/ARBXr159y+3/+9//xI4dO5pfT506VVQqleKFCxfM63766SdRoVCI2dnZoiiKYpMmTcTU1FSL48ycOVOMj48XRVEUz507JwIQDx48eMv3JSLH4jl7kq1169bBz88Per0eJpMJ//73vzFt2jTz9jZt2licp//9999x+vRp+Pv7WxynoqICZ86cQVFREbKzs9GpUyfzNpVKhbvvvvumofwqhw4dglKpRLdu3Woc9+nTp1FeXo4HH3zQYn1lZSU6dOgAADh+/LhFHAAQHx9f4/eo8tVXX2H+/Pk4c+YMSktLYTAYoNFoLNpER0ejQYMGFu9jMpmQkZEBf39/nDlzBsOGDcPw4cPNbQwGA7RardXxEJFjsNiTbPXo0QOLFi2Cp6cnIiIioFJZft19fX0tXpeWlqJjx45YsWLFTccKCQmpVQze3t5W71NaWgoA+PHHHy2KLHBtHoK97N69G4mJiZg+fToSEhKg1WqxcuVKvPvuu1bH+tFHH93040OpVNotViKyDYs9yZavry+aNm1a4/Z33XUXvvrqK4SGht7Uu61Sv3597N27F/fffz+Aaz3Y9PR03HXXXdW2b9OmDUwmE9LS0tCrV6+btleNLBiNRvO6uLg4qNVqZGZm3nJEoGXLlubJhlX27Nlz5yRvsGvXLsTExOC1114zr/vzzz9vapeZmYlLly4hIiLC/D4KhQKxsbEICwtDREQEzp49i8TERKven4jqDifoEf0tMTER9erVQ//+/bFjxw6cO3cO27Ztw8svv4wLFy4AAMaMGYM5c+ZgzZo1OHHiBF566aXbXiPfsGFDJCUl4fnnn8eaNWvMx1y1ahUAICYmBoIgYN26dcjPz0dpaSn8/f0xYcIEjBs3Dp9++inOnDmDAwcO4IMPPjBPenvxxRdx6tQpTJw4ERkZGUhNTcXy5cutyrdZs2bIzMzEypUrcebMGcyfP7/ayYZeXl5ISkrC77//jh07duDll1/GU089hfDwcADA9OnTkZKSgvnz5+PkyZM4fPgwli1bhrlz51oVDxE5Dos90d98fHywfft2REdHY8CAAWjZsiWGDRuGiooKc0//P//5DwYPHoykpCTEx8fD398fjz/++G2Pu2jRIjzxxBN46aWX0KJFCwwfPhxlZWUAgAYNGmD69Ol49dVXERYWhlGjRgEAZs6ciddffx0pKSlo2bIlHnroIfz4449o1KgRgGvn0b/99lusWbMG7dq1w+LFizF79myr8n300Ucxbtw4jBo1Cu3bt8euXbvw+uuv39SuadOmGDBgAB5++GH07t0bbdu2tbi07oUXXsDHH3+MZcuWoU2bNujWrRuWL19ujpWIpCeIt5pZRERERLLAnj0REZHMsdgTERHJHIs9ERGRzLHYExERyRyLPRERkcyx2BMREckciz0REZHMsdgTERHJHIs9ERGRzLHYExERyRyLPRERkcz9f03hud0yrsScAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm4 = confusion_matrix(y_test32,ensemble.predict(X_test32), labels=ensemble.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm4, display_labels=ensemble.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}